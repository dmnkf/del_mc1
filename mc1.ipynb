{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant for lightning.ai studio\n",
    "%cd del_mc1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=mc1.ipynb\n"
     ]
    }
   ],
   "source": [
    "# base libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ML related libraries\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "# todo do I need this?\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "# mlops\n",
    "import wandb\n",
    "\n",
    "%env WANDB_NOTEBOOK_NAME=mc1.ipynb\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## Tag ensure that wandb won't be cluttered\n",
    "DEVELOPMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training data\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "\n",
    "# Load CIFAR-10 test data\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "test_images, test_labels = next(iter(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print shapes of the datasets to verify\n",
    "print('train_images shape:', train_images.shape)\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print('test_images shape:', test_images.shape)\n",
    "print('test_labels shape:', test_labels.shape)\n",
    "\n",
    "# Accessing label names\n",
    "# get all labels \n",
    "label_names = train_dataset.classes\n",
    "print('label_names size:', len(label_names))\n",
    "print('label_names:', label_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis \n",
    "This section provides some insight into the CIFAR-10 dataset, which will help ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels, return_counts=True)[1], np.unique(test_labels, return_counts=True)[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are well distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_label_distribution(labels, label_names):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    for label, count in distribution.items():\n",
    "        print(f'{label_names[label]}: {count}')\n",
    "    return distribution\n",
    "\n",
    "def display_example_images(images, labels, label_names):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, label_name in enumerate(label_names):\n",
    "        index = np.where(labels == i)[0][0]\n",
    "        image = images[index].permute(1, 2, 0) \n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(label_name)\n",
    "        axes[i].title.set_size(20)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Training Labels Distribution:\")\n",
    "train_distribution = verify_label_distribution(train_labels, label_names)\n",
    "print(\"\\nTest Labels Distribution:\")\n",
    "test_distribution = verify_label_distribution(test_labels, label_names)\n",
    "\n",
    "print(\"\\nExample Images from Each Class:\")\n",
    "display_example_images(train_images, train_labels, label_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo maybe plot more images?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the mean and standard deviation for each color channel in the CIFAR-10 dataset.\n",
    "\n",
    "#### Reasoning\n",
    "This step is crucial for normalizing (relevant for preprocessing) the dataset, ensuring consistent model training and faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean: tensor([0.4914, 0.4822, 0.4465]), Training Std: tensor([0.2470, 0.2435, 0.2616])\n",
      "Test Mean: tensor([0.4942, 0.4851, 0.4504]), Test Std: tensor([0.2467, 0.2429, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_std(loader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "train_mean, train_std = calculate_mean_std(train_loader)\n",
    "test_mean, test_std = calculate_mean_std(test_loader)\n",
    "\n",
    "print(f\"Training Mean: {train_mean}, Training Std: {train_std}\")\n",
    "print(f\"Test Mean: {test_mean}, Test Std: {test_std}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot histograms of pixel values for the Red, Green, and Blue channels.\n",
    "\n",
    "#### Reasoning\n",
    "Understanding pixel intensity distribution aids in identifying dataset biases and informs necessary preprocessing adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_histograms(images):\n",
    "    images = images.permute(1, 0, 2, 3).reshape(3, -1)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, color in enumerate(['r', 'g', 'b']):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        # todo validate bins\n",
    "        plt.hist(images[i].numpy(), bins=20, color=color, alpha=0.7)\n",
    "        plt.title(f'{color.upper()} Channel')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xlabel('Pixel Intensity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_pixel_histograms(train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_distribution_per_class(images, labels, label_names):\n",
    "    num_classes = len(label_names)\n",
    "    fig, axes = plt.subplots(nrows=num_classes, ncols=3, figsize=(20, num_classes * 4))\n",
    "    \n",
    "    for i, label_name in enumerate(label_names):\n",
    "        class_images = images[labels == i]\n",
    "        # Flatten the images to a single dimension per channel for plotting\n",
    "        red_channel = class_images[:, 0, :, :].flatten()\n",
    "        green_channel = class_images[:, 1, :, :].flatten()\n",
    "        blue_channel = class_images[:, 2, :, :].flatten()\n",
    "        \n",
    "        for j, (channel, color) in enumerate(zip([red_channel, green_channel, blue_channel], ['Red', 'Green', 'Blue'])):\n",
    "            ax = axes[i, j]\n",
    "            ax.hist(channel.numpy(), bins=20, color=color.lower(), alpha=0.7)\n",
    "            ax.set_title(f'{label_name} - {color} Channel')\n",
    "            ax.set_xlabel('Pixel Intensity')\n",
    "            ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_color_distribution_per_class(train_images, train_labels, label_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the CIFAR-10 dataset is very well maintained (already labeled, pictures dimensions are consistent and distribution between labels is the same) the only thing we really need to do is properly normalize the data and do some train, validation and test split relevant for training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=train_mean, std=train_std), # using training mean and std for normalization\n",
    "])\n",
    "\n",
    "# Reload the datasets with normalization, keep it simple\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=normalize_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=normalize_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.2\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(validation_split * num_train))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling structure\n",
    "\n",
    "The metric used for evaluation in this challenge will be Accuracy. While simple, it is a good metric given that the dataset is very well balanced. \n",
    "\n",
    "Since this is a multi-class classification, using cross-entropy as the loss function is logical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, test_dataset, optimizer, criterion=nn.CrossEntropyLoss(), batch_size=32, epochs=10, seed=None, regularization=None, experiment=\"development\", track=True, run_name=''):\n",
    "\n",
    "        self.seed = seed    \n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.experiment = experiment\n",
    "\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        # reinitialize weights\n",
    "        self.model.initialize_weights()\n",
    "        self.regularization = regularization\n",
    "\n",
    "\n",
    "        self.config = self.auto_generate_config()\n",
    "        self.track = track\n",
    "        if self.track:\n",
    "            self.init_wandb()\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def auto_generate_config(self):\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        config = {\n",
    "            'model_type': self.model.__class__.__name__,\n",
    "            'experiment': self.experiment,\n",
    "            'optimizer': self.optimizer.__class__.__name__,\n",
    "            'learning_rate': lr,\n",
    "            'criterion': self.criterion.__class__.__name__,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'device': self.device,\n",
    "            'seed': self.seed,\n",
    "            'regularization': self.regularization\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def init_wandb(self):\n",
    "        run_name = f\"{self.config['model_type']}_LR{self.config['learning_rate']}_BS{self.config['batch_size']}_OP{self.config['optimizer']}_REG{self.config['regularization']}\"\n",
    "        group = f\"{self.config['experiment']}\"\n",
    "        wandb.init(project='del', name=run_name, group=group, config=self.config)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        total_steps = len(train_loader)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / total_steps\n",
    "            epoch_accuracy = correct_predictions / total_predictions\n",
    "            if self.track:\n",
    "                wandb.log({'epoch': epoch + 1, 'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy})\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        total_steps = len(test_loader)\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / total_steps\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        if self.track:\n",
    "            wandb.log({'test_loss': avg_loss, 'test_accuracy': accuracy})\n",
    "        print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def run(self):\n",
    "        self.train()\n",
    "        self.test()\n",
    "        \n",
    "        if self.track:\n",
    "            wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, test_dataset, optimizer, criterion=torch.nn.CrossEntropyLoss(), batch_size=32, epochs=40, seed=None, experiment=\"development\", track=True, run_name=''):\n",
    "        self.seed = seed    \n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.experiment = experiment\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.initialize_weights()\n",
    "\n",
    "        self.config = self.auto_generate_config()\n",
    "        self.track = track\n",
    "        if self.track:\n",
    "            self.run_name = run_name\n",
    "            self.init_wandb()\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def auto_generate_config(self):\n",
    "        self.lr = self.optimizer.param_groups[0]['lr']\n",
    "        config = {\n",
    "            'model_type': self.model.__class__.__name__,\n",
    "            'experiment': self.experiment,\n",
    "            'optimizer': self.optimizer.__class__.__name__,\n",
    "            'learning_rate': self.lr,\n",
    "            'criterion': self.criterion.__class__.__name__,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'device': self.device,\n",
    "            'seed': self.seed,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def init_wandb(self):\n",
    "        if not self.run_name:\n",
    "            self.run_name = f\"{self.config['model_type']}_LR{self.config['learning_rate']}_BS{self.config['batch_size']}\"\n",
    "\n",
    "        tags = []\n",
    "        if DEVELOPMENT:\n",
    "            tags = ['development']\n",
    "\n",
    "        wandb.init(project='del', name=self.run_name, group=self.config['experiment'], config=self.config, tags=tags)\n",
    "        wandb.watch(self.model, self.criterion, log='all', log_freq=10, log_graph=True)\n",
    "\n",
    "    def train(self, validate=False):\n",
    "        self.model.train()\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        total_steps = len(train_loader)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                # L1 reg is not supported by PyTorch optimizers natively\n",
    "                if hasattr(self.model, 'l1_penalty'):\n",
    "                    l1_loss = self.model.l1_penalty()\n",
    "                    loss += l1_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / total_steps\n",
    "            epoch_accuracy = correct_predictions / total_predictions\n",
    "            if self.track:\n",
    "                wandb.log({'epoch': epoch + 1, 'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy})\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "            if validate:\n",
    "                self.validate_epoch(epoch)\n",
    "\n",
    "    # todo this is 1:1 with test\n",
    "    def validate_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        valid_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(valid_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        if self.track:\n",
    "            wandb.log({'validation_loss': avg_loss, 'validation_accuracy': accuracy, 'epoch': epoch + 1})\n",
    "        print(f'Validation - Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(test_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        if self.track:\n",
    "            wandb.log({'test_loss': avg_loss, 'test_accuracy': accuracy})\n",
    "        print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def run(self, validate=True):\n",
    "        self.train(validate=validate)\n",
    "        # we either validate or test, as they share the same test set, so doing both is redundant.\n",
    "        if not validate:\n",
    "            self.test()\n",
    "        if self.track:\n",
    "            wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    " \n",
    "        if layers is None:\n",
    "            self.layers = [\n",
    "                nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                \n",
    "                nn.Conv2d(32, 96, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                # global pool\n",
    "                nn.AdaptiveMaxPool2d(1),              \n",
    "                nn.Flatten(),\n",
    "                nn.Linear(96, num_classes),\n",
    "            ]\n",
    "        else:\n",
    "            self.layers = layers\n",
    "\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "21"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240411_133543-9ta7c369</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/9ta7c369' target=\"_blank\">BasicCNN_LR0.01_BS32</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/9ta7c369' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/9ta7c369</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 5.7202, Accuracy: 0.1875\n",
      "Validation - Epoch [1/80], Loss: 5.0559, Accuracy: 0.0938\n",
      "Epoch [2/80], Loss: 4.8981, Accuracy: 0.0938\n",
      "Validation - Epoch [2/80], Loss: 4.2257, Accuracy: 0.0312\n",
      "Epoch [3/80], Loss: 3.4456, Accuracy: 0.1562\n",
      "Validation - Epoch [3/80], Loss: 3.2324, Accuracy: 0.0625\n",
      "Epoch [4/80], Loss: 2.5553, Accuracy: 0.2188\n",
      "Validation - Epoch [4/80], Loss: 2.9350, Accuracy: 0.0938\n",
      "Epoch [5/80], Loss: 2.1686, Accuracy: 0.3750\n",
      "Validation - Epoch [5/80], Loss: 3.0562, Accuracy: 0.0938\n",
      "Epoch [6/80], Loss: 2.0041, Accuracy: 0.2812\n",
      "Validation - Epoch [6/80], Loss: 2.8917, Accuracy: 0.1250\n",
      "Epoch [7/80], Loss: 1.9113, Accuracy: 0.3438\n",
      "Validation - Epoch [7/80], Loss: 3.0723, Accuracy: 0.0938\n",
      "Epoch [8/80], Loss: 1.8289, Accuracy: 0.3438\n",
      "Validation - Epoch [8/80], Loss: 2.8553, Accuracy: 0.1250\n",
      "Epoch [9/80], Loss: 1.7903, Accuracy: 0.4688\n",
      "Validation - Epoch [9/80], Loss: 3.0675, Accuracy: 0.0938\n",
      "Epoch [10/80], Loss: 1.6781, Accuracy: 0.4688\n",
      "Validation - Epoch [10/80], Loss: 2.8159, Accuracy: 0.1875\n",
      "Epoch [11/80], Loss: 1.6654, Accuracy: 0.5000\n",
      "Validation - Epoch [11/80], Loss: 3.0197, Accuracy: 0.1250\n",
      "Epoch [12/80], Loss: 1.5283, Accuracy: 0.4688\n",
      "Validation - Epoch [12/80], Loss: 2.7628, Accuracy: 0.2500\n",
      "Epoch [13/80], Loss: 1.5070, Accuracy: 0.5625\n",
      "Validation - Epoch [13/80], Loss: 2.9788, Accuracy: 0.1562\n",
      "Epoch [14/80], Loss: 1.3988, Accuracy: 0.4688\n",
      "Validation - Epoch [14/80], Loss: 2.7199, Accuracy: 0.2188\n",
      "Epoch [15/80], Loss: 1.3760, Accuracy: 0.6250\n",
      "Validation - Epoch [15/80], Loss: 2.9421, Accuracy: 0.1562\n",
      "Epoch [16/80], Loss: 1.2841, Accuracy: 0.5625\n",
      "Validation - Epoch [16/80], Loss: 2.6896, Accuracy: 0.2188\n",
      "Epoch [17/80], Loss: 1.2622, Accuracy: 0.6875\n",
      "Validation - Epoch [17/80], Loss: 2.9148, Accuracy: 0.1562\n",
      "Epoch [18/80], Loss: 1.1819, Accuracy: 0.6250\n",
      "Validation - Epoch [18/80], Loss: 2.6666, Accuracy: 0.1875\n",
      "Epoch [19/80], Loss: 1.1572, Accuracy: 0.7500\n",
      "Validation - Epoch [19/80], Loss: 2.8956, Accuracy: 0.1250\n",
      "Epoch [20/80], Loss: 1.0896, Accuracy: 0.7188\n",
      "Validation - Epoch [20/80], Loss: 2.6594, Accuracy: 0.1562\n",
      "Epoch [21/80], Loss: 1.0621, Accuracy: 0.8438\n",
      "Validation - Epoch [21/80], Loss: 2.8746, Accuracy: 0.1250\n",
      "Epoch [22/80], Loss: 1.0024, Accuracy: 0.8125\n",
      "Validation - Epoch [22/80], Loss: 2.6530, Accuracy: 0.1562\n",
      "Epoch [23/80], Loss: 0.9749, Accuracy: 0.8438\n",
      "Validation - Epoch [23/80], Loss: 2.8580, Accuracy: 0.1250\n",
      "Epoch [24/80], Loss: 0.9241, Accuracy: 0.8750\n",
      "Validation - Epoch [24/80], Loss: 2.6583, Accuracy: 0.1562\n",
      "Epoch [25/80], Loss: 0.8959, Accuracy: 0.8750\n",
      "Validation - Epoch [25/80], Loss: 2.8452, Accuracy: 0.1250\n",
      "Epoch [26/80], Loss: 0.8542, Accuracy: 0.9375\n",
      "Validation - Epoch [26/80], Loss: 2.6704, Accuracy: 0.1562\n",
      "Epoch [27/80], Loss: 0.8258, Accuracy: 0.9375\n",
      "Validation - Epoch [27/80], Loss: 2.8316, Accuracy: 0.0938\n",
      "Epoch [28/80], Loss: 0.7910, Accuracy: 0.9375\n",
      "Validation - Epoch [28/80], Loss: 2.6857, Accuracy: 0.1562\n",
      "Epoch [29/80], Loss: 0.7652, Accuracy: 1.0000\n",
      "Validation - Epoch [29/80], Loss: 2.8233, Accuracy: 0.0938\n",
      "Epoch [30/80], Loss: 0.7363, Accuracy: 0.9375\n",
      "Validation - Epoch [30/80], Loss: 2.7100, Accuracy: 0.1250\n",
      "Epoch [31/80], Loss: 0.7121, Accuracy: 1.0000\n",
      "Validation - Epoch [31/80], Loss: 2.8161, Accuracy: 0.0938\n",
      "Epoch [32/80], Loss: 0.6880, Accuracy: 1.0000\n",
      "Validation - Epoch [32/80], Loss: 2.7322, Accuracy: 0.1250\n",
      "Epoch [33/80], Loss: 0.6666, Accuracy: 1.0000\n",
      "Validation - Epoch [33/80], Loss: 2.8113, Accuracy: 0.1250\n",
      "Epoch [34/80], Loss: 0.6453, Accuracy: 1.0000\n",
      "Validation - Epoch [34/80], Loss: 2.7565, Accuracy: 0.1250\n",
      "Epoch [35/80], Loss: 0.6259, Accuracy: 1.0000\n",
      "Validation - Epoch [35/80], Loss: 2.8134, Accuracy: 0.1250\n",
      "Epoch [36/80], Loss: 0.6073, Accuracy: 1.0000\n",
      "Validation - Epoch [36/80], Loss: 2.7761, Accuracy: 0.1250\n",
      "Epoch [37/80], Loss: 0.5901, Accuracy: 1.0000\n",
      "Validation - Epoch [37/80], Loss: 2.8202, Accuracy: 0.1250\n",
      "Epoch [38/80], Loss: 0.5737, Accuracy: 1.0000\n",
      "Validation - Epoch [38/80], Loss: 2.7961, Accuracy: 0.1250\n",
      "Epoch [39/80], Loss: 0.5578, Accuracy: 1.0000\n",
      "Validation - Epoch [39/80], Loss: 2.8296, Accuracy: 0.1250\n",
      "Epoch [40/80], Loss: 0.5428, Accuracy: 1.0000\n",
      "Validation - Epoch [40/80], Loss: 2.8130, Accuracy: 0.1250\n",
      "Epoch [41/80], Loss: 0.5284, Accuracy: 1.0000\n",
      "Validation - Epoch [41/80], Loss: 2.8382, Accuracy: 0.1250\n",
      "Epoch [42/80], Loss: 0.5144, Accuracy: 1.0000\n",
      "Validation - Epoch [42/80], Loss: 2.8348, Accuracy: 0.1250\n",
      "Epoch [43/80], Loss: 0.5011, Accuracy: 1.0000\n",
      "Validation - Epoch [43/80], Loss: 2.8468, Accuracy: 0.1250\n",
      "Epoch [44/80], Loss: 0.4885, Accuracy: 1.0000\n",
      "Validation - Epoch [44/80], Loss: 2.8467, Accuracy: 0.1250\n",
      "Epoch [45/80], Loss: 0.4764, Accuracy: 1.0000\n",
      "Validation - Epoch [45/80], Loss: 2.8621, Accuracy: 0.1250\n",
      "Epoch [46/80], Loss: 0.4647, Accuracy: 1.0000\n",
      "Validation - Epoch [46/80], Loss: 2.8620, Accuracy: 0.1250\n",
      "Epoch [47/80], Loss: 0.4532, Accuracy: 1.0000\n",
      "Validation - Epoch [47/80], Loss: 2.8704, Accuracy: 0.1250\n",
      "Epoch [48/80], Loss: 0.4424, Accuracy: 1.0000\n",
      "Validation - Epoch [48/80], Loss: 2.8791, Accuracy: 0.1250\n",
      "Epoch [49/80], Loss: 0.4318, Accuracy: 1.0000\n",
      "Validation - Epoch [49/80], Loss: 2.8849, Accuracy: 0.1250\n",
      "Epoch [50/80], Loss: 0.4218, Accuracy: 1.0000\n",
      "Validation - Epoch [50/80], Loss: 2.8880, Accuracy: 0.1250\n",
      "Epoch [51/80], Loss: 0.4119, Accuracy: 1.0000\n",
      "Validation - Epoch [51/80], Loss: 2.8979, Accuracy: 0.1250\n",
      "Epoch [52/80], Loss: 0.4026, Accuracy: 1.0000\n",
      "Validation - Epoch [52/80], Loss: 2.9005, Accuracy: 0.1250\n",
      "Epoch [53/80], Loss: 0.3937, Accuracy: 1.0000\n",
      "Validation - Epoch [53/80], Loss: 2.9107, Accuracy: 0.1250\n",
      "Epoch [54/80], Loss: 0.3851, Accuracy: 1.0000\n",
      "Validation - Epoch [54/80], Loss: 2.9107, Accuracy: 0.1250\n",
      "Epoch [55/80], Loss: 0.3769, Accuracy: 1.0000\n",
      "Validation - Epoch [55/80], Loss: 2.9185, Accuracy: 0.1250\n",
      "Epoch [56/80], Loss: 0.3689, Accuracy: 1.0000\n",
      "Validation - Epoch [56/80], Loss: 2.9237, Accuracy: 0.1250\n",
      "Epoch [57/80], Loss: 0.3614, Accuracy: 1.0000\n",
      "Validation - Epoch [57/80], Loss: 2.9303, Accuracy: 0.1250\n",
      "Epoch [58/80], Loss: 0.3540, Accuracy: 1.0000\n",
      "Validation - Epoch [58/80], Loss: 2.9365, Accuracy: 0.1250\n",
      "Epoch [59/80], Loss: 0.3470, Accuracy: 1.0000\n",
      "Validation - Epoch [59/80], Loss: 2.9403, Accuracy: 0.1250\n",
      "Epoch [60/80], Loss: 0.3401, Accuracy: 1.0000\n",
      "Validation - Epoch [60/80], Loss: 2.9474, Accuracy: 0.1250\n",
      "Epoch [61/80], Loss: 0.3336, Accuracy: 1.0000\n",
      "Validation - Epoch [61/80], Loss: 2.9553, Accuracy: 0.1250\n",
      "Epoch [62/80], Loss: 0.3272, Accuracy: 1.0000\n",
      "Validation - Epoch [62/80], Loss: 2.9587, Accuracy: 0.1250\n",
      "Epoch [63/80], Loss: 0.3210, Accuracy: 1.0000\n",
      "Validation - Epoch [63/80], Loss: 2.9649, Accuracy: 0.1250\n",
      "Epoch [64/80], Loss: 0.3149, Accuracy: 1.0000\n",
      "Validation - Epoch [64/80], Loss: 2.9710, Accuracy: 0.1250\n",
      "Epoch [65/80], Loss: 0.3092, Accuracy: 1.0000\n",
      "Validation - Epoch [65/80], Loss: 2.9739, Accuracy: 0.1250\n",
      "Epoch [66/80], Loss: 0.3036, Accuracy: 1.0000\n",
      "Validation - Epoch [66/80], Loss: 2.9800, Accuracy: 0.1250\n",
      "Epoch [67/80], Loss: 0.2982, Accuracy: 1.0000\n",
      "Validation - Epoch [67/80], Loss: 2.9863, Accuracy: 0.1250\n",
      "Epoch [68/80], Loss: 0.2930, Accuracy: 1.0000\n",
      "Validation - Epoch [68/80], Loss: 2.9885, Accuracy: 0.1250\n",
      "Epoch [69/80], Loss: 0.2878, Accuracy: 1.0000\n",
      "Validation - Epoch [69/80], Loss: 2.9953, Accuracy: 0.1250\n",
      "Epoch [70/80], Loss: 0.2828, Accuracy: 1.0000\n",
      "Validation - Epoch [70/80], Loss: 2.9995, Accuracy: 0.1250\n",
      "Epoch [71/80], Loss: 0.2779, Accuracy: 1.0000\n",
      "Validation - Epoch [71/80], Loss: 3.0045, Accuracy: 0.1250\n",
      "Epoch [72/80], Loss: 0.2732, Accuracy: 1.0000\n",
      "Validation - Epoch [72/80], Loss: 3.0101, Accuracy: 0.1250\n",
      "Epoch [73/80], Loss: 0.2687, Accuracy: 1.0000\n",
      "Validation - Epoch [73/80], Loss: 3.0146, Accuracy: 0.1250\n",
      "Epoch [74/80], Loss: 0.2642, Accuracy: 1.0000\n",
      "Validation - Epoch [74/80], Loss: 3.0195, Accuracy: 0.1250\n",
      "Epoch [75/80], Loss: 0.2598, Accuracy: 1.0000\n",
      "Validation - Epoch [75/80], Loss: 3.0230, Accuracy: 0.1250\n",
      "Epoch [76/80], Loss: 0.2555, Accuracy: 1.0000\n",
      "Validation - Epoch [76/80], Loss: 3.0292, Accuracy: 0.1250\n",
      "Epoch [77/80], Loss: 0.2514, Accuracy: 1.0000\n",
      "Validation - Epoch [77/80], Loss: 3.0317, Accuracy: 0.1250\n",
      "Epoch [78/80], Loss: 0.2474, Accuracy: 1.0000\n",
      "Validation - Epoch [78/80], Loss: 3.0378, Accuracy: 0.1250\n",
      "Epoch [79/80], Loss: 0.2436, Accuracy: 1.0000\n",
      "Validation - Epoch [79/80], Loss: 3.0413, Accuracy: 0.1250\n",
      "Epoch [80/80], Loss: 0.2398, Accuracy: 1.0000\n",
      "Validation - Epoch [80/80], Loss: 3.0483, Accuracy: 0.1250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca140d78edf34bc793baf1586b820d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▃▃▄▄▄▅▅▆▇▇▇▇██████████████████████████</td></tr><tr><td>train_loss</td><td>█▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▃▁▃▃▃▆███▆▆▆▆▃▃▃▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>validation_loss</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>80</td></tr><tr><td>train_accuracy</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.23978</td></tr><tr><td>validation_accuracy</td><td>0.125</td></tr><tr><td>validation_loss</td><td>3.04828</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BasicCNN_LR0.01_BS32</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/9ta7c369' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/9ta7c369</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240411_133543-9ta7c369/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n",
    "model = BasicCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(batch_size))\n",
    "test_subset = torch.utils.data.Subset(test_dataset, range(batch_size))\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_subset, test_dataset=test_subset, optimizer=optimizer, experiment='overfit', epochs=50, batch_size=batch_size, seed=55, track=True)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only a subset of 10 images and batch size 5 on 20 epochs quickly shows that the SimpleCNN model is overfitting, which makes sense given the training data size.\n",
    "\n",
    "On the test set we can see the consequences of the overfitting with a poor accuracy of about 14%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SGD, without REG, without BN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weight Initialisation\n",
    "\n",
    "Weight initialisation refers to the process of assigning initial values to the weights of the network's neurons. It can have a great impact on the learning process and performance of the model. \n",
    "\n",
    "Keywords:\n",
    "- Vanishing Gradient\n",
    "- Convergence speed\n",
    "- Local Minima\n",
    "\n",
    "Over the past years standards have established themselves based on the activation function in use.\n",
    "\n",
    "\n",
    "#### todo this requires more love.... it is not as clear as I would like it to be.\n",
    "\n",
    "Fortunately PyTorch has good defaults, and automatically uses the He Initialisation for Conv2d with LeakyReLU activation functions.\n",
    "\n",
    "Source:\n",
    "https://arc.net/l/quote/wfgutzqj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "## START WRONG weight initialization\n",
    "\n",
    "class OneWeightCNN(BasicCNN):\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All weights verified as 1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240411_133649-g762rn41</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/g762rn41' target=\"_blank\">OneWeightCNN</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/g762rn41' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/g762rn41</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 9745.1190, Accuracy: 0.1003\n",
      "Validation - Epoch [1/40], Loss: 2.3027, Accuracy: 0.1000\n",
      "Epoch [2/40], Loss: 2.3027, Accuracy: 0.0995\n",
      "Validation - Epoch [2/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [3/40], Loss: 2.3027, Accuracy: 0.0973\n",
      "Validation - Epoch [3/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [4/40], Loss: 2.3027, Accuracy: 0.0975\n",
      "Validation - Epoch [4/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [5/40], Loss: 2.3027, Accuracy: 0.0960\n",
      "Validation - Epoch [5/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [6/40], Loss: 2.3027, Accuracy: 0.0992\n",
      "Validation - Epoch [6/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [7/40], Loss: 2.3027, Accuracy: 0.0961\n",
      "Validation - Epoch [7/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [8/40], Loss: 2.3027, Accuracy: 0.0982\n",
      "Validation - Epoch [8/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [9/40], Loss: 2.3027, Accuracy: 0.0998\n",
      "Validation - Epoch [9/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [10/40], Loss: 2.3027, Accuracy: 0.0976\n",
      "Validation - Epoch [10/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [11/40], Loss: 2.3027, Accuracy: 0.0985\n",
      "Validation - Epoch [11/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [12/40], Loss: 2.3027, Accuracy: 0.0993\n",
      "Validation - Epoch [12/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [13/40], Loss: 2.3027, Accuracy: 0.0965\n",
      "Validation - Epoch [13/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [14/40], Loss: 2.3027, Accuracy: 0.0994\n",
      "Validation - Epoch [14/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [15/40], Loss: 2.3027, Accuracy: 0.0980\n",
      "Validation - Epoch [15/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [16/40], Loss: 2.3027, Accuracy: 0.0975\n",
      "Validation - Epoch [16/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [17/40], Loss: 2.3027, Accuracy: 0.0973\n",
      "Validation - Epoch [17/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [18/40], Loss: 2.3027, Accuracy: 0.0982\n",
      "Validation - Epoch [18/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [19/40], Loss: 2.3027, Accuracy: 0.0970\n",
      "Validation - Epoch [19/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [20/40], Loss: 2.3027, Accuracy: 0.0989\n",
      "Validation - Epoch [20/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [21/40], Loss: 2.3027, Accuracy: 0.0956\n",
      "Validation - Epoch [21/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [22/40], Loss: 2.3027, Accuracy: 0.0979\n",
      "Validation - Epoch [22/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [23/40], Loss: 2.3027, Accuracy: 0.0972\n",
      "Validation - Epoch [23/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [24/40], Loss: 2.3027, Accuracy: 0.0955\n",
      "Validation - Epoch [24/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [25/40], Loss: 2.3027, Accuracy: 0.0964\n",
      "Validation - Epoch [25/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [26/40], Loss: 2.3027, Accuracy: 0.0985\n",
      "Validation - Epoch [26/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [27/40], Loss: 2.3027, Accuracy: 0.0969\n",
      "Validation - Epoch [27/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [28/40], Loss: 2.3027, Accuracy: 0.0980\n",
      "Validation - Epoch [28/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [29/40], Loss: 2.3027, Accuracy: 0.0985\n",
      "Validation - Epoch [29/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [30/40], Loss: 2.3027, Accuracy: 0.0968\n",
      "Validation - Epoch [30/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [31/40], Loss: 2.3027, Accuracy: 0.0999\n",
      "Validation - Epoch [31/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [32/40], Loss: 2.3027, Accuracy: 0.0994\n",
      "Validation - Epoch [32/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [33/40], Loss: 2.3027, Accuracy: 0.0993\n",
      "Validation - Epoch [33/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [34/40], Loss: 2.3027, Accuracy: 0.0978\n",
      "Validation - Epoch [34/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [35/40], Loss: 2.3027, Accuracy: 0.0974\n",
      "Validation - Epoch [35/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [36/40], Loss: 2.3027, Accuracy: 0.0979\n",
      "Validation - Epoch [36/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [37/40], Loss: 2.3027, Accuracy: 0.0964\n",
      "Validation - Epoch [37/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [38/40], Loss: 2.3027, Accuracy: 0.0972\n",
      "Validation - Epoch [38/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [39/40], Loss: 2.3027, Accuracy: 0.0983\n",
      "Validation - Epoch [39/40], Loss: 2.3026, Accuracy: 0.1000\n",
      "Epoch [40/40], Loss: 2.3027, Accuracy: 0.0986\n",
      "Validation - Epoch [40/40], Loss: 2.3026, Accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935d90a768fc4428b1f9da0604649ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>█▇▄▄▂▆▂▅▇▄▅▆▂▇▅▄▄▅▃▆▁▄▃▁▂▅▃▅▅▃▇▇▇▄▄▄▂▄▅▆</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▂▁▂▂▁▁▁▂▂▂▂▁▁▃▁▁▁▁▁▁▁▂▁▂▂▂▁▂▂▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_accuracy</td><td>0.09864</td></tr><tr><td>train_loss</td><td>2.3027</td></tr><tr><td>validation_accuracy</td><td>0.1</td></tr><tr><td>validation_loss</td><td>2.3026</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">OneWeightCNN</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/g762rn41' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/g762rn41</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240411_133649-g762rn41/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = OneWeightCNN()\n",
    "# check if all weights are 1\n",
    "model.initialize_weights()\n",
    "\n",
    "# Validate that all weights are set to 1\n",
    "for name, param in model.named_parameters():\n",
    "    if \"weight\" in name:  # Ensure we're only checking weights, not biases\n",
    "        assert torch.all(param == 1), f\"{name} not all ones\"\n",
    "print(\"All weights verified as 1.\")\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#todo enable tracking\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', seed=55, track=True, run_name='OneWeightCNN')\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "## Do RANDOM weight initialization\n",
    "class RandomWeightCNN(BasicCNN):\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                # Initialize weights with a uniform distribution\n",
    "                nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240411_140918-ewp4t43i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/ewp4t43i' target=\"_blank\">RandomWeightCNN</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/ewp4t43i' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/ewp4t43i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 2.0966, Accuracy: 0.2390\n",
      "Validation - Epoch [1/40], Loss: 1.8695, Accuracy: 0.3399\n",
      "Epoch [2/40], Loss: 1.7547, Accuracy: 0.3708\n",
      "Validation - Epoch [2/40], Loss: 1.6447, Accuracy: 0.4062\n",
      "Epoch [3/40], Loss: 1.5437, Accuracy: 0.4488\n",
      "Validation - Epoch [3/40], Loss: 1.4743, Accuracy: 0.4628\n",
      "Epoch [4/40], Loss: 1.4151, Accuracy: 0.4988\n",
      "Validation - Epoch [4/40], Loss: 1.4215, Accuracy: 0.4854\n",
      "Epoch [5/40], Loss: 1.3342, Accuracy: 0.5267\n",
      "Validation - Epoch [5/40], Loss: 1.3302, Accuracy: 0.5193\n",
      "Epoch [6/40], Loss: 1.2777, Accuracy: 0.5504\n",
      "Validation - Epoch [6/40], Loss: 1.3339, Accuracy: 0.5213\n",
      "Epoch [7/40], Loss: 1.2310, Accuracy: 0.5652\n",
      "Validation - Epoch [7/40], Loss: 1.3426, Accuracy: 0.5070\n",
      "Epoch [8/40], Loss: 1.1926, Accuracy: 0.5813\n",
      "Validation - Epoch [8/40], Loss: 1.2544, Accuracy: 0.5548\n",
      "Epoch [9/40], Loss: 1.1583, Accuracy: 0.5933\n",
      "Validation - Epoch [9/40], Loss: 1.2002, Accuracy: 0.5708\n",
      "Epoch [10/40], Loss: 1.1283, Accuracy: 0.6064\n",
      "Validation - Epoch [10/40], Loss: 1.1575, Accuracy: 0.5887\n",
      "Epoch [11/40], Loss: 1.0996, Accuracy: 0.6151\n",
      "Validation - Epoch [11/40], Loss: 1.1735, Accuracy: 0.5896\n",
      "Epoch [12/40], Loss: 1.0758, Accuracy: 0.6228\n",
      "Validation - Epoch [12/40], Loss: 1.2236, Accuracy: 0.5552\n",
      "Epoch [13/40], Loss: 1.0509, Accuracy: 0.6343\n",
      "Validation - Epoch [13/40], Loss: 1.1301, Accuracy: 0.5968\n",
      "Epoch [14/40], Loss: 1.0287, Accuracy: 0.6416\n",
      "Validation - Epoch [14/40], Loss: 1.1461, Accuracy: 0.5889\n",
      "Epoch [15/40], Loss: 1.0127, Accuracy: 0.6458\n",
      "Validation - Epoch [15/40], Loss: 1.1802, Accuracy: 0.5900\n",
      "Epoch [16/40], Loss: 0.9932, Accuracy: 0.6547\n",
      "Validation - Epoch [16/40], Loss: 1.0663, Accuracy: 0.6253\n",
      "Epoch [17/40], Loss: 0.9746, Accuracy: 0.6609\n",
      "Validation - Epoch [17/40], Loss: 1.1364, Accuracy: 0.5998\n",
      "Epoch [18/40], Loss: 0.9582, Accuracy: 0.6696\n",
      "Validation - Epoch [18/40], Loss: 1.0641, Accuracy: 0.6294\n",
      "Epoch [19/40], Loss: 0.9422, Accuracy: 0.6719\n",
      "Validation - Epoch [19/40], Loss: 1.0621, Accuracy: 0.6321\n",
      "Epoch [20/40], Loss: 0.9285, Accuracy: 0.6767\n",
      "Validation - Epoch [20/40], Loss: 1.0937, Accuracy: 0.6240\n",
      "Epoch [21/40], Loss: 0.9120, Accuracy: 0.6821\n",
      "Validation - Epoch [21/40], Loss: 1.0365, Accuracy: 0.6412\n",
      "Epoch [22/40], Loss: 0.9005, Accuracy: 0.6868\n",
      "Validation - Epoch [22/40], Loss: 1.0688, Accuracy: 0.6242\n",
      "Epoch [23/40], Loss: 0.8869, Accuracy: 0.6913\n",
      "Validation - Epoch [23/40], Loss: 1.0450, Accuracy: 0.6409\n",
      "Epoch [24/40], Loss: 0.8748, Accuracy: 0.6954\n",
      "Validation - Epoch [24/40], Loss: 1.0302, Accuracy: 0.6475\n",
      "Epoch [25/40], Loss: 0.8621, Accuracy: 0.6997\n",
      "Validation - Epoch [25/40], Loss: 1.0394, Accuracy: 0.6429\n",
      "Epoch [26/40], Loss: 0.8536, Accuracy: 0.7000\n",
      "Validation - Epoch [26/40], Loss: 0.9765, Accuracy: 0.6660\n",
      "Epoch [27/40], Loss: 0.8457, Accuracy: 0.7047\n",
      "Validation - Epoch [27/40], Loss: 1.1070, Accuracy: 0.6249\n",
      "Epoch [28/40], Loss: 0.8310, Accuracy: 0.7105\n",
      "Validation - Epoch [28/40], Loss: 1.0218, Accuracy: 0.6541\n",
      "Epoch [29/40], Loss: 0.8239, Accuracy: 0.7137\n",
      "Validation - Epoch [29/40], Loss: 0.9592, Accuracy: 0.6690\n",
      "Epoch [30/40], Loss: 0.8131, Accuracy: 0.7172\n",
      "Validation - Epoch [30/40], Loss: 0.9997, Accuracy: 0.6548\n",
      "Epoch [31/40], Loss: 0.8070, Accuracy: 0.7179\n",
      "Validation - Epoch [31/40], Loss: 1.0670, Accuracy: 0.6382\n",
      "Epoch [32/40], Loss: 0.7972, Accuracy: 0.7221\n",
      "Validation - Epoch [32/40], Loss: 1.0060, Accuracy: 0.6586\n",
      "Epoch [33/40], Loss: 0.7882, Accuracy: 0.7239\n",
      "Validation - Epoch [33/40], Loss: 1.0044, Accuracy: 0.6596\n",
      "Epoch [34/40], Loss: 0.7824, Accuracy: 0.7273\n",
      "Validation - Epoch [34/40], Loss: 1.0143, Accuracy: 0.6588\n",
      "Epoch [35/40], Loss: 0.7730, Accuracy: 0.7308\n",
      "Validation - Epoch [35/40], Loss: 0.9417, Accuracy: 0.6769\n",
      "Epoch [36/40], Loss: 0.7703, Accuracy: 0.7297\n",
      "Validation - Epoch [36/40], Loss: 0.9818, Accuracy: 0.6634\n",
      "Epoch [37/40], Loss: 0.7594, Accuracy: 0.7362\n",
      "Validation - Epoch [37/40], Loss: 0.9831, Accuracy: 0.6697\n",
      "Epoch [38/40], Loss: 0.7543, Accuracy: 0.7361\n",
      "Validation - Epoch [38/40], Loss: 1.0304, Accuracy: 0.6683\n",
      "Epoch [39/40], Loss: 0.7463, Accuracy: 0.7390\n",
      "Validation - Epoch [39/40], Loss: 1.0594, Accuracy: 0.6483\n",
      "Epoch [40/40], Loss: 0.7404, Accuracy: 0.7400\n",
      "Validation - Epoch [40/40], Loss: 0.9786, Accuracy: 0.6727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121ed461f0a841f291b950fae9e3529b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▄▄▅▅▄▅▆▆▆▅▆▆▆▇▆▇▇▇▇▇▇▇▇█▇███▇███████▇█</td></tr><tr><td>validation_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▂▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_accuracy</td><td>0.73996</td></tr><tr><td>train_loss</td><td>0.74036</td></tr><tr><td>validation_accuracy</td><td>0.6727</td></tr><tr><td>validation_loss</td><td>0.97857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RandomWeightCNN</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/ewp4t43i' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/ewp4t43i</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240411_140918-ewp4t43i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = RandomWeightCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', seed=55, track=True, run_name='RandomWeightCNN')\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240411_144158-pkdd3s5z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/pkdd3s5z' target=\"_blank\">LeakyReluWeightCNN</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/pkdd3s5z' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/pkdd3s5z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 1.8108, Accuracy: 0.3547\n",
      "Validation - Epoch [1/40], Loss: 1.5663, Accuracy: 0.4370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Loss: 1.4574, Accuracy: 0.4819\n",
      "Validation - Epoch [2/40], Loss: 1.4797, Accuracy: 0.4646\n",
      "Epoch [3/40], Loss: 1.3272, Accuracy: 0.5291\n",
      "Validation - Epoch [3/40], Loss: 1.3726, Accuracy: 0.5052\n",
      "Epoch [4/40], Loss: 1.2481, Accuracy: 0.5613\n",
      "Validation - Epoch [4/40], Loss: 1.3166, Accuracy: 0.5284\n",
      "Epoch [5/40], Loss: 1.1923, Accuracy: 0.5804\n",
      "Validation - Epoch [5/40], Loss: 1.2498, Accuracy: 0.5517\n",
      "Epoch [6/40], Loss: 1.1475, Accuracy: 0.5968\n",
      "Validation - Epoch [6/40], Loss: 1.2736, Accuracy: 0.5483\n",
      "Epoch [7/40], Loss: 1.1100, Accuracy: 0.6117\n",
      "Validation - Epoch [7/40], Loss: 1.2665, Accuracy: 0.5484\n",
      "Epoch [8/40], Loss: 1.0785, Accuracy: 0.6193\n",
      "Validation - Epoch [8/40], Loss: 1.1687, Accuracy: 0.5888\n",
      "Epoch [9/40], Loss: 1.0510, Accuracy: 0.6301\n",
      "Validation - Epoch [9/40], Loss: 1.1788, Accuracy: 0.5748\n",
      "Epoch [10/40], Loss: 1.0283, Accuracy: 0.6380\n",
      "Validation - Epoch [10/40], Loss: 1.1064, Accuracy: 0.6066\n",
      "Epoch [11/40], Loss: 1.0047, Accuracy: 0.6461\n",
      "Validation - Epoch [11/40], Loss: 1.0846, Accuracy: 0.6216\n",
      "Epoch [12/40], Loss: 0.9842, Accuracy: 0.6544\n",
      "Validation - Epoch [12/40], Loss: 1.1547, Accuracy: 0.5889\n",
      "Epoch [13/40], Loss: 0.9648, Accuracy: 0.6595\n",
      "Validation - Epoch [13/40], Loss: 1.0993, Accuracy: 0.6131\n",
      "Epoch [14/40], Loss: 0.9461, Accuracy: 0.6672\n",
      "Validation - Epoch [14/40], Loss: 1.0651, Accuracy: 0.6243\n",
      "Epoch [15/40], Loss: 0.9321, Accuracy: 0.6715\n",
      "Validation - Epoch [15/40], Loss: 1.1789, Accuracy: 0.5867\n",
      "Epoch [16/40], Loss: 0.9170, Accuracy: 0.6779\n",
      "Validation - Epoch [16/40], Loss: 1.0760, Accuracy: 0.6271\n",
      "Epoch [17/40], Loss: 0.9026, Accuracy: 0.6822\n",
      "Validation - Epoch [17/40], Loss: 1.0972, Accuracy: 0.6188\n",
      "Epoch [18/40], Loss: 0.8881, Accuracy: 0.6865\n",
      "Validation - Epoch [18/40], Loss: 1.0967, Accuracy: 0.6207\n",
      "Epoch [19/40], Loss: 0.8784, Accuracy: 0.6928\n",
      "Validation - Epoch [19/40], Loss: 1.0864, Accuracy: 0.6247\n",
      "Epoch [20/40], Loss: 0.8665, Accuracy: 0.6951\n",
      "Validation - Epoch [20/40], Loss: 1.0972, Accuracy: 0.6231\n",
      "Epoch [21/40], Loss: 0.8512, Accuracy: 0.7010\n",
      "Validation - Epoch [21/40], Loss: 1.0476, Accuracy: 0.6329\n",
      "Epoch [22/40], Loss: 0.8428, Accuracy: 0.7042\n",
      "Validation - Epoch [22/40], Loss: 1.0865, Accuracy: 0.6202\n"
     ]
    }
   ],
   "source": [
    "## Do Default\n",
    "model = BasicCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', epochs=epochs, batch_size=batch_size, seed=55, track=True, run_name=\"HeWeightCNN\")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "class HeWeightCNN(BasicCNN):\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d5d30637c24a0a8abc2e78404b8842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112408900064312, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240410_224445-582x4xgm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/582x4xgm' target=\"_blank\">HeWeightCNN</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/582x4xgm' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/582x4xgm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 1.5761, Accuracy: 0.4736\n",
      "Validation - Epoch [1/40], Loss: 1.3386, Accuracy: 0.5385\n",
      "Epoch [2/40], Loss: 1.1850, Accuracy: 0.5867\n",
      "Validation - Epoch [2/40], Loss: 1.1699, Accuracy: 0.5943\n",
      "Epoch [3/40], Loss: 1.0765, Accuracy: 0.6278\n",
      "Validation - Epoch [3/40], Loss: 1.1894, Accuracy: 0.5870\n",
      "Epoch [4/40], Loss: 1.0116, Accuracy: 0.6496\n",
      "Validation - Epoch [4/40], Loss: 1.1180, Accuracy: 0.6118\n",
      "Epoch [5/40], Loss: 0.9619, Accuracy: 0.6713\n",
      "Validation - Epoch [5/40], Loss: 1.0904, Accuracy: 0.6269\n",
      "Epoch [6/40], Loss: 0.9269, Accuracy: 0.6812\n",
      "Validation - Epoch [6/40], Loss: 1.0566, Accuracy: 0.6424\n",
      "Epoch [7/40], Loss: 0.8975, Accuracy: 0.6920\n",
      "Validation - Epoch [7/40], Loss: 1.1283, Accuracy: 0.6159\n",
      "Epoch [8/40], Loss: 0.8715, Accuracy: 0.7008\n",
      "Validation - Epoch [8/40], Loss: 1.0881, Accuracy: 0.6384\n",
      "Epoch [9/40], Loss: 0.8516, Accuracy: 0.7074\n",
      "Validation - Epoch [9/40], Loss: 1.0831, Accuracy: 0.6280\n",
      "Epoch [10/40], Loss: 0.8303, Accuracy: 0.7133\n",
      "Validation - Epoch [10/40], Loss: 1.1739, Accuracy: 0.6119\n",
      "Epoch [11/40], Loss: 0.8101, Accuracy: 0.7230\n",
      "Validation - Epoch [11/40], Loss: 1.0625, Accuracy: 0.6430\n",
      "Epoch [12/40], Loss: 0.7963, Accuracy: 0.7262\n",
      "Validation - Epoch [12/40], Loss: 1.1137, Accuracy: 0.6326\n",
      "Epoch [13/40], Loss: 0.7792, Accuracy: 0.7329\n",
      "Validation - Epoch [13/40], Loss: 1.0860, Accuracy: 0.6406\n",
      "Epoch [14/40], Loss: 0.7652, Accuracy: 0.7370\n",
      "Validation - Epoch [14/40], Loss: 1.1055, Accuracy: 0.6283\n",
      "Epoch [15/40], Loss: 0.7506, Accuracy: 0.7432\n",
      "Validation - Epoch [15/40], Loss: 1.0988, Accuracy: 0.6343\n",
      "Epoch [16/40], Loss: 0.7395, Accuracy: 0.7451\n",
      "Validation - Epoch [16/40], Loss: 1.1299, Accuracy: 0.6278\n",
      "Epoch [17/40], Loss: 0.7270, Accuracy: 0.7493\n",
      "Validation - Epoch [17/40], Loss: 1.1078, Accuracy: 0.6342\n",
      "Epoch [18/40], Loss: 0.7135, Accuracy: 0.7560\n",
      "Validation - Epoch [18/40], Loss: 1.1093, Accuracy: 0.6397\n",
      "Epoch [19/40], Loss: 0.7042, Accuracy: 0.7574\n",
      "Validation - Epoch [19/40], Loss: 1.1218, Accuracy: 0.6367\n",
      "Epoch [20/40], Loss: 0.6928, Accuracy: 0.7609\n",
      "Validation - Epoch [20/40], Loss: 1.1671, Accuracy: 0.6286\n",
      "Epoch [21/40], Loss: 0.6853, Accuracy: 0.7667\n",
      "Validation - Epoch [21/40], Loss: 1.1623, Accuracy: 0.6330\n",
      "Epoch [22/40], Loss: 0.6742, Accuracy: 0.7674\n",
      "Validation - Epoch [22/40], Loss: 1.1534, Accuracy: 0.6341\n",
      "Epoch [23/40], Loss: 0.6643, Accuracy: 0.7719\n",
      "Validation - Epoch [23/40], Loss: 1.1912, Accuracy: 0.6242\n",
      "Epoch [24/40], Loss: 0.6576, Accuracy: 0.7741\n",
      "Validation - Epoch [24/40], Loss: 1.1745, Accuracy: 0.6256\n",
      "Epoch [25/40], Loss: 0.6469, Accuracy: 0.7753\n",
      "Validation - Epoch [25/40], Loss: 1.1562, Accuracy: 0.6410\n",
      "Epoch [26/40], Loss: 0.6393, Accuracy: 0.7797\n",
      "Validation - Epoch [26/40], Loss: 1.2353, Accuracy: 0.6194\n",
      "Epoch [27/40], Loss: 0.6292, Accuracy: 0.7815\n",
      "Validation - Epoch [27/40], Loss: 1.1917, Accuracy: 0.6316\n",
      "Epoch [28/40], Loss: 0.6229, Accuracy: 0.7837\n",
      "Validation - Epoch [28/40], Loss: 1.2153, Accuracy: 0.6308\n",
      "Epoch [29/40], Loss: 0.6150, Accuracy: 0.7884\n",
      "Validation - Epoch [29/40], Loss: 1.2483, Accuracy: 0.6210\n",
      "Epoch [30/40], Loss: 0.6042, Accuracy: 0.7910\n",
      "Validation - Epoch [30/40], Loss: 1.2378, Accuracy: 0.6321\n",
      "Epoch [31/40], Loss: 0.6007, Accuracy: 0.7934\n",
      "Validation - Epoch [31/40], Loss: 1.3036, Accuracy: 0.6139\n",
      "Epoch [32/40], Loss: 0.5942, Accuracy: 0.7945\n",
      "Validation - Epoch [32/40], Loss: 1.2522, Accuracy: 0.6297\n",
      "Epoch [33/40], Loss: 0.5881, Accuracy: 0.7970\n",
      "Validation - Epoch [33/40], Loss: 1.2351, Accuracy: 0.6312\n",
      "Epoch [34/40], Loss: 0.5796, Accuracy: 0.8002\n",
      "Validation - Epoch [34/40], Loss: 1.2561, Accuracy: 0.6283\n",
      "Epoch [35/40], Loss: 0.5746, Accuracy: 0.8023\n",
      "Validation - Epoch [35/40], Loss: 1.2742, Accuracy: 0.6268\n",
      "Epoch [36/40], Loss: 0.5677, Accuracy: 0.8043\n",
      "Validation - Epoch [36/40], Loss: 1.2513, Accuracy: 0.6320\n",
      "Epoch [37/40], Loss: 0.5619, Accuracy: 0.8040\n",
      "Validation - Epoch [37/40], Loss: 1.2800, Accuracy: 0.6187\n",
      "Epoch [38/40], Loss: 0.5554, Accuracy: 0.8068\n",
      "Validation - Epoch [38/40], Loss: 1.3692, Accuracy: 0.6138\n",
      "Epoch [39/40], Loss: 0.5494, Accuracy: 0.8107\n",
      "Validation - Epoch [39/40], Loss: 1.3197, Accuracy: 0.6206\n",
      "Epoch [40/40], Loss: 0.5428, Accuracy: 0.8132\n",
      "Validation - Epoch [40/40], Loss: 1.4113, Accuracy: 0.6014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5874d6697a3c40fc9f006217c77be6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>train_loss</td><td>█▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▄▆▇█▆█▇▆█▇█▇▇▇▇██▇▇▇▇▇█▆▇▇▇▇▆▇▇▇▇▇▆▆▆▅</td></tr><tr><td>validation_loss</td><td>▇▃▄▂▂▁▂▂▂▃▁▂▂▂▂▂▂▂▂▃▃▃▄▃▃▅▄▄▅▅▆▅▅▅▅▅▅▇▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_accuracy</td><td>0.8132</td></tr><tr><td>train_loss</td><td>0.54282</td></tr><tr><td>validation_accuracy</td><td>0.6014</td></tr><tr><td>validation_loss</td><td>1.41133</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">HeWeightCNN</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/582x4xgm' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/582x4xgm</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_224445-582x4xgm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HeWeightCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## todo Default \n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', seed=55, track=True, run_name='D')\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/dmnkf/del/reports/--weight_init?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7fbd9193df60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot wandb weight_init report\n",
    "\n",
    "%wandb dmnkf/del/reports/weight_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results of the report we will change the default weight init of Conv2d and Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Für jedes Modell mit gegebener Anzahl Layer und Units pro Layer führe ein sorgfältiges Hyper-Parameter-Tuning durch. Untersuche, wie sich die das Training verändert bei unterschiedlicher Wahl für die Lernrate, in einer separaten Betrachtung auch für die Batch-Grösse. Achte stets darauf, dass das Training stabil läuft. Merke Dir bei jedem Training, den Loss, die Performance Metrik(en) inkl. Schätzfehler, die verwendete Anzahl Epochen, Lernrate und Batch-Grösse. Beachte: Keine Verfahren zur automatischen Hyperparameter-Suche (z.B. kein Bayesian und kein Random Parameter- Sweep Methoden) verwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [1, 0.1, 0.01, 0.001, 0.00001]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = BasicCNN()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='learning_rate', seed=55, track=True, run_name=f\"BasicCNN_LR{learning_rate}\")\n",
    "    trainer.run(validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "batch_sizes = [1000, 64, 32, 16, 1]\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    model = BasicCNN()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='batch_size', batch_size=bs, seed=55, track=True, run_name=f\"BasicCNN_BS{batch_size}\")\n",
    "    trainer.run(validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "Variiere die Anzahl Layer und Anzahl Units pro Layer, um eine möglichst gute Performance zu erreichen. Falls auch CNNs (ohne Transfer-Learning) verwendet werden variiere auch Anzahl Filter, Kernel-Grösse, Stride, Padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "To get started, we will look at the impact of different layers in our neural network model. We will experiment with varying the number of layers and observe how it affects the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CNN_3Conv(BasicCNN):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layers = [\n",
    "            nn.Conv2d(self.input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        ]\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "class CNN_4Conv(BasicCNN):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layers = [\n",
    "            nn.Conv2d(self.input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        ]\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "\n",
    "class CNN_6Conv(BasicCNN):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layers = [\n",
    "            nn.Conv2d(self.input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                \n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                \n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_classes),\n",
    "        ]\n",
    "        self.features = nn.Sequential(*self.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "conv_models = {\n",
    "    'CNN_3Conv': CNN_3Conv,\n",
    "    'CNN_4Conv': CNN_4Conv,\n",
    "    'CNN_6Conv': CNN_6Conv\n",
    "}\n",
    "\n",
    "for model_name, model_class in conv_models.items():\n",
    "    model = model_class()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='conv_layers', epochs=epochs, batch_size=batch_size, seed=55, track=True, run_name=model_name)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel size in a convolution layer plays a significant role in determining the feature extraction capabilities of the layer. The kernel size refers to the width and height of the filter used in the convolution operation.\n",
    "\n",
    "When the kernel size is increased, the convolution layer is able to capture more global or spatial information from the input. This can be beneficial when the important features in the data are spread out or when the data has a high degree of variability. However, a larger kernel size also means more parameters to learn, which can increase the computational complexity and the risk of overfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Ziehe nun verschiedene Regularisierungsmethoden bei den MLP Layern in Betracht:\n",
    "a. L1/L2 Weight Penalty\n",
    "b. Dropout\n",
    "Evaluiere den Nutzen der Regularisierung, auch unter Berücksichtigung verschiedener Regularisierungsstärken.\n",
    "Beschreibe auch kurz, was allgemein das Ziel von Regularisierungsmethoden ist (Regularisierung im Allgemeinen, sowie auch Idee der einzelnen Methoden). Inwiefern wird dieses Ziel im gegebenen Fall erreicht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# regularization is done by optimizer so using the basicCNN model seems valid\n",
    "model = BasicCNN(input_channels=3, num_classes=10)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01) \n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='regularization', seed=55, track=True, run_name='BasicCNN_REGL2')\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNRegL1(BasicCNN):\n",
    "    def __init__(self, l1_strength=0.0005, **kwargs):\n",
    "        super(CNNRegL1, self).__init__(**kwargs)\n",
    "        self.l1_strength = l1_strength\n",
    "\n",
    "    def l1_penalty(self):\n",
    "        \"\"\"\n",
    "        Calculate the L1 penalty for the model's weights only, excluding biases.\n",
    "        This method iterates over all parameters that require gradients and have more than one dimension,\n",
    "        which generally corresponds to the weights of the model.\n",
    "        \"\"\"\n",
    "        l1_norm = sum(p.abs().sum() for p in self.parameters() if p.requires_grad and len(p.shape) > 1)\n",
    "        return self.l1_strength * l1_norm\n",
    "\n",
    "model = CNNRegL1(input_channels=3, num_classes=10, l1_strength=0.0005)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='regularization', seed=55, track=True, run_name='BasicCNN_REGL1')\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutCNN(BasicCNN):\n",
    "    def __init__(self, *args, dropout_rate=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = [\n",
    "            nn.Conv2d(self.input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((8, 8)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128 * 8 * 8, self.num_classes)\n",
    "        ]\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "        \n",
    "model = DropoutCNN(input_channels=3, num_classes=10, dropout_rate=0.5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='regularization', epochs=15, batch_size=32, seed=55, track=True, run_name='DropoutCNN')\n",
    "trainer.run(validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Batchnorm  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo common thing to do something completely different here I think, not sure anymore, sigmoid??\n",
    "class BatchNormCNN(BasicCNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = [\n",
    "            nn.Conv2d(self.input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, self.num_classes)\n",
    "        ]\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "\n",
    "model = BatchNormCNN(input_channels=3, num_classes=10)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='batchnorm', seed=55, track=True, run_name='BatchNormCNN')\n",
    "trainer.run(validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BasicCNN for Adam optimization, no changes to the model structure are needed.\n",
    "model = BasicCNN(input_channels=3, num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='adam', seed=55, track=True)\n",
    "trainer.run(validate=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
