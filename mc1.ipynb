{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/del_mc1\n"
     ]
    }
   ],
   "source": [
    "# relevant for lightning.ai studio\n",
    "%cd del_mc1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=mc1.ipynb\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# base libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ML related libraries\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "# todo do I need this?\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "import os\n",
    "\n",
    "# mlops\n",
    "import wandb\n",
    "\n",
    "%env WANDB_NOTEBOOK_NAME=mc1.ipynb\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "## Tag ensure that wandb won't be cluttered\n",
    "DEVELOPMENT = True\n",
    "CUSTOM_TAGS = ['draft_5']\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training data\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "\n",
    "# Load CIFAR-10 test data\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "test_images, test_labels = next(iter(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print shapes of the datasets to verify\n",
    "print('train_images shape:', train_images.shape)\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print('test_images shape:', test_images.shape)\n",
    "print('test_labels shape:', test_labels.shape)\n",
    "\n",
    "# Accessing label names\n",
    "# get all labels \n",
    "label_names = train_dataset.classes\n",
    "print('label_names size:', len(label_names))\n",
    "print('label_names:', label_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis \n",
    "This section provides some insight into the CIFAR-10 dataset, which will help ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels, return_counts=True)[1], np.unique(test_labels, return_counts=True)[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are well distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_label_distribution(labels, label_names):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    for label, count in distribution.items():\n",
    "        print(f'{label_names[label]}: {count}')\n",
    "    return distribution\n",
    "\n",
    "def display_example_images(images, labels, label_names):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, label_name in enumerate(label_names):\n",
    "        index = np.where(labels == i)[0][0]\n",
    "        image = images[index].permute(1, 2, 0) \n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(label_name)\n",
    "        axes[i].title.set_size(20)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training Labels Distribution:\")\n",
    "train_distribution = verify_label_distribution(train_labels, label_names)\n",
    "print(\"\\nTest Labels Distribution:\")\n",
    "test_distribution = verify_label_distribution(test_labels, label_names)\n",
    "\n",
    "print(\"\\nExample Images from Each Class:\")\n",
    "display_example_images(train_images, train_labels, label_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo maybe plot more images?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the mean and standard deviation for each color channel in the CIFAR-10 dataset.\n",
    "\n",
    "#### Reasoning\n",
    "This step is crucial for normalizing (relevant for preprocessing) the dataset, ensuring consistent model training and faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean: tensor([0.4914, 0.4822, 0.4465]), Training Std: tensor([0.2470, 0.2435, 0.2616])\n",
      "Test Mean: tensor([0.4942, 0.4851, 0.4504]), Test Std: tensor([0.2467, 0.2429, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_std(loader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "train_mean, train_std = calculate_mean_std(train_loader)\n",
    "test_mean, test_std = calculate_mean_std(test_loader)\n",
    "\n",
    "print(f\"Training Mean: {train_mean}, Training Std: {train_std}\")\n",
    "print(f\"Test Mean: {test_mean}, Test Std: {test_std}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot histograms of pixel values for the Red, Green, and Blue channels.\n",
    "\n",
    "#### Reasoning\n",
    "Understanding pixel intensity distribution aids in identifying dataset biases and informs necessary preprocessing adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_histograms(images):\n",
    "    images = images.permute(1, 0, 2, 3).reshape(3, -1)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, color in enumerate(['r', 'g', 'b']):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        # todo validate bins\n",
    "        plt.hist(images[i].numpy(), bins=20, color=color, alpha=0.7)\n",
    "        plt.title(f'{color.upper()} Channel')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xlabel('Pixel Intensity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_pixel_histograms(train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_distribution_per_class(images, labels, label_names):\n",
    "    num_classes = len(label_names)\n",
    "    fig, axes = plt.subplots(nrows=num_classes, ncols=3, figsize=(20, num_classes * 4))\n",
    "    \n",
    "    for i, label_name in enumerate(label_names):\n",
    "        class_images = images[labels == i]\n",
    "        # Flatten the images to a single dimension per channel for plotting\n",
    "        red_channel = class_images[:, 0, :, :].flatten()\n",
    "        green_channel = class_images[:, 1, :, :].flatten()\n",
    "        blue_channel = class_images[:, 2, :, :].flatten()\n",
    "        \n",
    "        for j, (channel, color) in enumerate(zip([red_channel, green_channel, blue_channel], ['Red', 'Green', 'Blue'])):\n",
    "            ax = axes[i, j]\n",
    "            ax.hist(channel.numpy(), bins=20, color=color.lower(), alpha=0.7)\n",
    "            ax.set_title(f'{label_name} - {color} Channel')\n",
    "            ax.set_xlabel('Pixel Intensity')\n",
    "            ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_color_distribution_per_class(train_images, train_labels, label_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the CIFAR-10 dataset is very well maintained (already labeled, pictures dimensions are consistent and distribution between labels is the same) the only thing we really need to do is properly normalize the data and do some train, validation and test split relevant for training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=train_mean, std=train_std), # using training mean and std for normalization\n",
    "])\n",
    "\n",
    "# Reload the datasets with normalization, keep it simple\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=normalize_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=normalize_transform)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling structure\n",
    "\n",
    "The metric used for evaluation in this challenge will be Accuracy. While simple, it is a good metric given that the dataset is very well balanced. \n",
    "\n",
    "Since this is a multi-class classification, using cross-entropy as the loss function is logical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmnkf\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.login()\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_dataset, test_dataset, optimizer, criterion=torch.nn.CrossEntropyLoss(), batch_size=128, epochs=55, seed=None, experiment=\"development\", track=True, run_name=''):\n",
    "        self.seed = seed    \n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.experiment = experiment\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.initialize_weights()\n",
    "\n",
    "        self.config = self.auto_generate_config()\n",
    "        self.track = track\n",
    "        if self.track:\n",
    "            self.run_name = run_name\n",
    "            self.init_wandb()\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def auto_generate_config(self):\n",
    "        self.lr = self.optimizer.param_groups[0]['lr']\n",
    "        config = {\n",
    "            'model_type': self.model.__class__.__name__,\n",
    "            'experiment': self.experiment,\n",
    "            'optimizer': self.optimizer.__class__.__name__,\n",
    "            'learning_rate': self.lr,\n",
    "            'criterion': self.criterion.__class__.__name__,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'device': self.device,\n",
    "            'seed': self.seed,\n",
    "            'trainable_params': sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def init_wandb(self):\n",
    "        if not self.run_name:\n",
    "            self.run_name = f\"{self.config['model_type']}_LR{self.config['learning_rate']}_BS{self.config['batch_size']}\"\n",
    "\n",
    "        tags = [self.config['experiment']]\n",
    "        if DEVELOPMENT:\n",
    "            tags.append('development')\n",
    "        \n",
    "        if CUSTOM_TAGS:\n",
    "            tags.extend(CUSTOM_TAGS)\n",
    "\n",
    "        wandb.init(project='del', name=self.run_name, group=self.config['experiment'], config=self.config, tags=tags)\n",
    "        wandb.watch(self.model, self.criterion, log='all', log_freq=10, log_graph=True)\n",
    "\n",
    "    def train(self, validate=False):\n",
    "        self.model.train()\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        total_steps = len(train_loader)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                # L1 reg is not supported by PyTorch optimizers natively\n",
    "                if hasattr(self.model, 'l1_penalty'):\n",
    "                    l1_loss = self.model.l1_penalty()\n",
    "                    loss += l1_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / total_steps\n",
    "            epoch_accuracy = correct_predictions / total_predictions\n",
    "            if self.track:\n",
    "                wandb.log({'epoch': epoch + 1, 'train_loss': epoch_loss, 'train_accuracy': epoch_accuracy})\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "            if validate:\n",
    "                self.validate_epoch(epoch)\n",
    "\n",
    "    # todo this is 1:1 with test\n",
    "    def validate_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        valid_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(valid_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        if self.track:\n",
    "            wandb.log({'validation_loss': avg_loss, 'validation_accuracy': accuracy, 'epoch': epoch + 1})\n",
    "        print(f'Validation - Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(test_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        if self.track:\n",
    "            wandb.log({'test_loss': avg_loss, 'test_accuracy': accuracy})\n",
    "        print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def run(self, validate=True):\n",
    "        self.train(validate=validate)\n",
    "        # we either validate or test, as they share the same test set, so doing both is redundant.\n",
    "        if not validate:\n",
    "            self.test()\n",
    "        if self.track:\n",
    "            w_run = wandb.run\n",
    "            wandb.finish()\n",
    "            return w_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    " \n",
    "        if layers is None:\n",
    "            self.layers = [\n",
    "                nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                \n",
    "                nn.Conv2d(32, 96, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(96 * 8 * 8, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, num_classes),\n",
    "            ]\n",
    "        else:\n",
    "            self.layers = layers\n",
    "\n",
    "        self.features = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):             \n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            elif hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total trainable features\n",
    "model = BasicCNN()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total Trainable Parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BasicCNN` class provided here is designed as a simple convolutional neural network (CNN) suitable for image classification tasks. The model architecture is chosen to offer a balanced approach between complexity and performance, making it well-suited for tasks that do not require extremely deep networks. It will serve as the base foundation for all following experiments and analysis.\n",
    "\n",
    "1. **Layer Configuration**: It starts with an initial convolutional layer that takes in images with `input_channels` (defaulting to 3 for RGB images) and outputs 32 feature maps, using a kernel size of 3x3, stride of 1, and padding of 1 to preserve spatial dimensions. This is followed by a ReLU activation function for non-linearity and a max pooling layer to reduce the spatial dimensions by half, enhancing feature extraction while reducing computational load.\n",
    "\n",
    "2. **Further Convolution and Pooling**: The process is repeated with another convolutional layer reducing the feature maps to 16, followed by ReLU and max pooling. This further helps in capturing more abstract features from the input images while continually reducing data dimensionality.\n",
    "\n",
    "3. **Flattening and Output**: The network flattens the output of the last pooling layer and feeds it into a fully connected (linear) layer that maps the features to the `num_classes`, which represent the final classification scores for each class.\n",
    "\n",
    "4. **Weight Initialization**: The model uses He initialization (`kaiming_uniform_`) for convolutional and linear layers to ensure the initial weights are scaled appropriately, reducing the chance of vanishing or exploding gradients, especially important in networks with ReLU activations. (comparison follows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit check\n",
    "\n",
    "To ensure that the model is capable of learning, the following block will train on a single batch (32 images) for train and test. The expectation here is that the model should overfit the train dataset quite easily hitting a perfect accuracy of 1.0. The train loss should smoothly converge to 0, whereas the validation loss will most likely increase over time. However given the small subset, the validation loss and accuracy should be bad as the model should overfit on train data and pick up every bit of noise so that it no longer is capable of generalizing unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/del_mc1/wandb/run-20240415_200035-5756x1si</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/5756x1si' target=\"_blank\">BasicCNN_LR0.001_BS32</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/5756x1si' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/5756x1si</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 3.8694, Accuracy: 0.0625\n",
      "Validation - Epoch [1/30], Loss: 3.3277, Accuracy: 0.1250\n",
      "Epoch [2/30], Loss: 2.7890, Accuracy: 0.1875\n",
      "Validation - Epoch [2/30], Loss: 3.1339, Accuracy: 0.1562\n",
      "Epoch [3/30], Loss: 2.4539, Accuracy: 0.2500\n",
      "Validation - Epoch [3/30], Loss: 3.0606, Accuracy: 0.1562\n",
      "Epoch [4/30], Loss: 2.2021, Accuracy: 0.2812\n",
      "Validation - Epoch [4/30], Loss: 2.9917, Accuracy: 0.1875\n",
      "Epoch [5/30], Loss: 2.0029, Accuracy: 0.3750\n",
      "Validation - Epoch [5/30], Loss: 2.9584, Accuracy: 0.1875\n",
      "Epoch [6/30], Loss: 1.8376, Accuracy: 0.4062\n",
      "Validation - Epoch [6/30], Loss: 2.9064, Accuracy: 0.1562\n",
      "Epoch [7/30], Loss: 1.6954, Accuracy: 0.4062\n",
      "Validation - Epoch [7/30], Loss: 2.8724, Accuracy: 0.1875\n",
      "Epoch [8/30], Loss: 1.5761, Accuracy: 0.3750\n",
      "Validation - Epoch [8/30], Loss: 2.8494, Accuracy: 0.1562\n",
      "Epoch [9/30], Loss: 1.4744, Accuracy: 0.4688\n",
      "Validation - Epoch [9/30], Loss: 2.8341, Accuracy: 0.1562\n",
      "Epoch [10/30], Loss: 1.3847, Accuracy: 0.5938\n",
      "Validation - Epoch [10/30], Loss: 2.8239, Accuracy: 0.1875\n",
      "Epoch [11/30], Loss: 1.3067, Accuracy: 0.6250\n",
      "Validation - Epoch [11/30], Loss: 2.8227, Accuracy: 0.1562\n",
      "Epoch [12/30], Loss: 1.2356, Accuracy: 0.6562\n",
      "Validation - Epoch [12/30], Loss: 2.8276, Accuracy: 0.1875\n",
      "Epoch [13/30], Loss: 1.1724, Accuracy: 0.6875\n",
      "Validation - Epoch [13/30], Loss: 2.8281, Accuracy: 0.1875\n",
      "Epoch [14/30], Loss: 1.1156, Accuracy: 0.7188\n",
      "Validation - Epoch [14/30], Loss: 2.8340, Accuracy: 0.1875\n",
      "Epoch [15/30], Loss: 1.0648, Accuracy: 0.7812\n",
      "Validation - Epoch [15/30], Loss: 2.8394, Accuracy: 0.1875\n",
      "Epoch [16/30], Loss: 1.0178, Accuracy: 0.8438\n",
      "Validation - Epoch [16/30], Loss: 2.8468, Accuracy: 0.1875\n",
      "Epoch [17/30], Loss: 0.9750, Accuracy: 0.8750\n",
      "Validation - Epoch [17/30], Loss: 2.8619, Accuracy: 0.1875\n",
      "Epoch [18/30], Loss: 0.9347, Accuracy: 0.9062\n",
      "Validation - Epoch [18/30], Loss: 2.8702, Accuracy: 0.1875\n",
      "Epoch [19/30], Loss: 0.8969, Accuracy: 0.9062\n",
      "Validation - Epoch [19/30], Loss: 2.8751, Accuracy: 0.1875\n",
      "Epoch [20/30], Loss: 0.8611, Accuracy: 0.9062\n",
      "Validation - Epoch [20/30], Loss: 2.8898, Accuracy: 0.1875\n",
      "Epoch [21/30], Loss: 0.8287, Accuracy: 0.9062\n",
      "Validation - Epoch [21/30], Loss: 2.8970, Accuracy: 0.1875\n",
      "Epoch [22/30], Loss: 0.7970, Accuracy: 0.9062\n",
      "Validation - Epoch [22/30], Loss: 2.9048, Accuracy: 0.1875\n",
      "Epoch [23/30], Loss: 0.7679, Accuracy: 0.9062\n",
      "Validation - Epoch [23/30], Loss: 2.9261, Accuracy: 0.1875\n",
      "Epoch [24/30], Loss: 0.7383, Accuracy: 0.9062\n",
      "Validation - Epoch [24/30], Loss: 2.9333, Accuracy: 0.1875\n",
      "Epoch [25/30], Loss: 0.7119, Accuracy: 0.9062\n",
      "Validation - Epoch [25/30], Loss: 2.9492, Accuracy: 0.1875\n",
      "Epoch [26/30], Loss: 0.6875, Accuracy: 0.9688\n",
      "Validation - Epoch [26/30], Loss: 2.9678, Accuracy: 0.1875\n",
      "Epoch [27/30], Loss: 0.6644, Accuracy: 0.9688\n",
      "Validation - Epoch [27/30], Loss: 2.9774, Accuracy: 0.1875\n",
      "Epoch [28/30], Loss: 0.6425, Accuracy: 0.9688\n",
      "Validation - Epoch [28/30], Loss: 2.9910, Accuracy: 0.1875\n",
      "Epoch [29/30], Loss: 0.6220, Accuracy: 0.9688\n",
      "Validation - Epoch [29/30], Loss: 3.0036, Accuracy: 0.1875\n",
      "Epoch [30/30], Loss: 0.6026, Accuracy: 1.0000\n",
      "Validation - Epoch [30/30], Loss: 3.0178, Accuracy: 0.1875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934384e2638a453ab9d79207d1618a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▄▄▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▅██▅█▅▅█▅███████████████████</td></tr><tr><td>validation_loss</td><td>█▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.60264</td></tr><tr><td>validation_accuracy</td><td>0.1875</td></tr><tr><td>validation_loss</td><td>3.01776</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BasicCNN_LR0.001_BS32</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/5756x1si' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/5756x1si</a><br/> View job at <a href='https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v233' target=\"_blank\">https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v233</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240415_200035-5756x1si/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dmnkf/del/runs/5756x1si?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f78d9bc7e80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 32\n",
    "\n",
    "model = BasicCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(bs))\n",
    "test_subset = torch.utils.data.Subset(test_dataset, range(bs))\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_subset, test_dataset=test_subset, optimizer=optimizer, experiment='overfit', epochs=30, batch_size=bs, seed=55, track=True)\n",
    "\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://wandb.ai/dmnkf/del/reports/overfit--Vmlldzo3NTIzMDc4\" style=\"border:none;height:1024px;width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The results do align with my theory and understanding. Looking at the train loss we can see how it quickly converges and is hitting a perfect accuracy on the train dataset by epoch 20. The validation loss on the other hand does increase as train loss decreases, futher indicating strong overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "\n",
    "For the upcoming experiments and analysis, we will be training the BasicCNN once to serve as baseline for comparisons at each point. The results of this training will be included in every following report of the notebook. Doing this here improves the overall training time as we are reuse the results from this run instead of rerunning it at every step.\n",
    "\n",
    "The defaults defined here will be used for all upcoming runs if not specified differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cfkjda47) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a7bb438bd3421e9235c67f531f5ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▄▅▆▆▆▇██</td></tr><tr><td>validation_loss</td><td>█▇▅▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.66244</td></tr><tr><td>train_loss</td><td>0.98152</td></tr><tr><td>validation_accuracy</td><td>0.6253</td></tr><tr><td>validation_loss</td><td>1.07294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_LR0.001_BS32</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/cfkjda47' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/cfkjda47</a><br/> View job at <a href='https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v233' target=\"_blank\">https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v233</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240415_200147-cfkjda47/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cfkjda47). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cee3d1257141e3bf9e1fc902ad6e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112327088888681, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/del_mc1/wandb/run-20240415_201626-ou93mfod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/ou93mfod' target=\"_blank\">baseline_LR0.001_BS128</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/ou93mfod' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/ou93mfod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/55], Loss: 1.9839, Accuracy: 0.2995\n",
      "Validation - Epoch [1/55], Loss: 1.7816, Accuracy: 0.3657\n",
      "Epoch [2/55], Loss: 1.7136, Accuracy: 0.4002\n",
      "Validation - Epoch [2/55], Loss: 1.6730, Accuracy: 0.4089\n",
      "Epoch [3/55], Loss: 1.6097, Accuracy: 0.4362\n",
      "Validation - Epoch [3/55], Loss: 1.5843, Accuracy: 0.4429\n",
      "Epoch [4/55], Loss: 1.5393, Accuracy: 0.4627\n",
      "Validation - Epoch [4/55], Loss: 1.5468, Accuracy: 0.4513\n",
      "Epoch [5/55], Loss: 1.4843, Accuracy: 0.4855\n",
      "Validation - Epoch [5/55], Loss: 1.4749, Accuracy: 0.4817\n",
      "Epoch [6/55], Loss: 1.4390, Accuracy: 0.4999\n",
      "Validation - Epoch [6/55], Loss: 1.4472, Accuracy: 0.4907\n",
      "Epoch [7/55], Loss: 1.3999, Accuracy: 0.5133\n",
      "Validation - Epoch [7/55], Loss: 1.4214, Accuracy: 0.4982\n",
      "Epoch [8/55], Loss: 1.3643, Accuracy: 0.5268\n",
      "Validation - Epoch [8/55], Loss: 1.3857, Accuracy: 0.5112\n",
      "Epoch [9/55], Loss: 1.3332, Accuracy: 0.5374\n",
      "Validation - Epoch [9/55], Loss: 1.3458, Accuracy: 0.5279\n",
      "Epoch [10/55], Loss: 1.3048, Accuracy: 0.5463\n",
      "Validation - Epoch [10/55], Loss: 1.3216, Accuracy: 0.5337\n",
      "Epoch [11/55], Loss: 1.2772, Accuracy: 0.5581\n",
      "Validation - Epoch [11/55], Loss: 1.3087, Accuracy: 0.5378\n",
      "Epoch [12/55], Loss: 1.2542, Accuracy: 0.5652\n",
      "Validation - Epoch [12/55], Loss: 1.2946, Accuracy: 0.5393\n",
      "Epoch [13/55], Loss: 1.2310, Accuracy: 0.5726\n",
      "Validation - Epoch [13/55], Loss: 1.2765, Accuracy: 0.5469\n",
      "Epoch [14/55], Loss: 1.2084, Accuracy: 0.5810\n",
      "Validation - Epoch [14/55], Loss: 1.2453, Accuracy: 0.5540\n",
      "Epoch [15/55], Loss: 1.1896, Accuracy: 0.5864\n",
      "Validation - Epoch [15/55], Loss: 1.2472, Accuracy: 0.5588\n",
      "Epoch [16/55], Loss: 1.1706, Accuracy: 0.5937\n",
      "Validation - Epoch [16/55], Loss: 1.2229, Accuracy: 0.5641\n",
      "Epoch [17/55], Loss: 1.1529, Accuracy: 0.5996\n",
      "Validation - Epoch [17/55], Loss: 1.2063, Accuracy: 0.5695\n",
      "Epoch [18/55], Loss: 1.1376, Accuracy: 0.6064\n",
      "Validation - Epoch [18/55], Loss: 1.1901, Accuracy: 0.5763\n",
      "Epoch [19/55], Loss: 1.1214, Accuracy: 0.6128\n",
      "Validation - Epoch [19/55], Loss: 1.1837, Accuracy: 0.5776\n",
      "Epoch [20/55], Loss: 1.1070, Accuracy: 0.6172\n",
      "Validation - Epoch [20/55], Loss: 1.1862, Accuracy: 0.5828\n",
      "Epoch [21/55], Loss: 1.0933, Accuracy: 0.6229\n",
      "Validation - Epoch [21/55], Loss: 1.1701, Accuracy: 0.5819\n",
      "Epoch [22/55], Loss: 1.0799, Accuracy: 0.6258\n",
      "Validation - Epoch [22/55], Loss: 1.1613, Accuracy: 0.5874\n",
      "Epoch [23/55], Loss: 1.0676, Accuracy: 0.6328\n",
      "Validation - Epoch [23/55], Loss: 1.1675, Accuracy: 0.5839\n",
      "Epoch [24/55], Loss: 1.0555, Accuracy: 0.6363\n",
      "Validation - Epoch [24/55], Loss: 1.1447, Accuracy: 0.5972\n",
      "Epoch [25/55], Loss: 1.0430, Accuracy: 0.6412\n",
      "Validation - Epoch [25/55], Loss: 1.1326, Accuracy: 0.6003\n",
      "Epoch [26/55], Loss: 1.0332, Accuracy: 0.6439\n",
      "Validation - Epoch [26/55], Loss: 1.1271, Accuracy: 0.5988\n",
      "Epoch [27/55], Loss: 1.0214, Accuracy: 0.6480\n",
      "Validation - Epoch [27/55], Loss: 1.1104, Accuracy: 0.6057\n",
      "Epoch [28/55], Loss: 1.0114, Accuracy: 0.6535\n",
      "Validation - Epoch [28/55], Loss: 1.1067, Accuracy: 0.6085\n",
      "Epoch [29/55], Loss: 1.0010, Accuracy: 0.6560\n",
      "Validation - Epoch [29/55], Loss: 1.1090, Accuracy: 0.6051\n",
      "Epoch [30/55], Loss: 0.9898, Accuracy: 0.6616\n",
      "Validation - Epoch [30/55], Loss: 1.0986, Accuracy: 0.6090\n",
      "Epoch [31/55], Loss: 0.9810, Accuracy: 0.6641\n",
      "Validation - Epoch [31/55], Loss: 1.0872, Accuracy: 0.6168\n",
      "Epoch [32/55], Loss: 0.9716, Accuracy: 0.6674\n",
      "Validation - Epoch [32/55], Loss: 1.0814, Accuracy: 0.6158\n",
      "Epoch [33/55], Loss: 0.9626, Accuracy: 0.6703\n",
      "Validation - Epoch [33/55], Loss: 1.0893, Accuracy: 0.6154\n",
      "Epoch [34/55], Loss: 0.9534, Accuracy: 0.6751\n",
      "Validation - Epoch [34/55], Loss: 1.0780, Accuracy: 0.6222\n",
      "Epoch [35/55], Loss: 0.9447, Accuracy: 0.6783\n",
      "Validation - Epoch [35/55], Loss: 1.0690, Accuracy: 0.6216\n",
      "Epoch [36/55], Loss: 0.9361, Accuracy: 0.6814\n",
      "Validation - Epoch [36/55], Loss: 1.0751, Accuracy: 0.6233\n",
      "Epoch [37/55], Loss: 0.9293, Accuracy: 0.6841\n",
      "Validation - Epoch [37/55], Loss: 1.0671, Accuracy: 0.6271\n",
      "Epoch [38/55], Loss: 0.9201, Accuracy: 0.6857\n",
      "Validation - Epoch [38/55], Loss: 1.0727, Accuracy: 0.6205\n",
      "Epoch [39/55], Loss: 0.9118, Accuracy: 0.6905\n",
      "Validation - Epoch [39/55], Loss: 1.0545, Accuracy: 0.6272\n",
      "Epoch [40/55], Loss: 0.9042, Accuracy: 0.6925\n",
      "Validation - Epoch [40/55], Loss: 1.0448, Accuracy: 0.6322\n",
      "Epoch [41/55], Loss: 0.8965, Accuracy: 0.6967\n",
      "Validation - Epoch [41/55], Loss: 1.0480, Accuracy: 0.6326\n",
      "Epoch [42/55], Loss: 0.8892, Accuracy: 0.6985\n",
      "Validation - Epoch [42/55], Loss: 1.0581, Accuracy: 0.6280\n",
      "Epoch [43/55], Loss: 0.8811, Accuracy: 0.7011\n",
      "Validation - Epoch [43/55], Loss: 1.0440, Accuracy: 0.6353\n",
      "Epoch [44/55], Loss: 0.8750, Accuracy: 0.7035\n",
      "Validation - Epoch [44/55], Loss: 1.0415, Accuracy: 0.6345\n",
      "Epoch [45/55], Loss: 0.8670, Accuracy: 0.7046\n",
      "Validation - Epoch [45/55], Loss: 1.0573, Accuracy: 0.6270\n",
      "Epoch [46/55], Loss: 0.8598, Accuracy: 0.7093\n",
      "Validation - Epoch [46/55], Loss: 1.0502, Accuracy: 0.6323\n",
      "Epoch [47/55], Loss: 0.8517, Accuracy: 0.7102\n",
      "Validation - Epoch [47/55], Loss: 1.0295, Accuracy: 0.6389\n",
      "Epoch [48/55], Loss: 0.8462, Accuracy: 0.7144\n",
      "Validation - Epoch [48/55], Loss: 1.0231, Accuracy: 0.6426\n",
      "Epoch [49/55], Loss: 0.8395, Accuracy: 0.7162\n",
      "Validation - Epoch [49/55], Loss: 1.0187, Accuracy: 0.6413\n",
      "Epoch [50/55], Loss: 0.8325, Accuracy: 0.7183\n",
      "Validation - Epoch [50/55], Loss: 1.0449, Accuracy: 0.6356\n",
      "Epoch [51/55], Loss: 0.8265, Accuracy: 0.7197\n",
      "Validation - Epoch [51/55], Loss: 1.0195, Accuracy: 0.6435\n",
      "Epoch [52/55], Loss: 0.8194, Accuracy: 0.7228\n",
      "Validation - Epoch [52/55], Loss: 1.0129, Accuracy: 0.6427\n",
      "Epoch [53/55], Loss: 0.8143, Accuracy: 0.7252\n",
      "Validation - Epoch [53/55], Loss: 1.0335, Accuracy: 0.6405\n",
      "Epoch [54/55], Loss: 0.8082, Accuracy: 0.7276\n",
      "Validation - Epoch [54/55], Loss: 1.0140, Accuracy: 0.6419\n",
      "Epoch [55/55], Loss: 0.8005, Accuracy: 0.7305\n",
      "Validation - Epoch [55/55], Loss: 1.0285, Accuracy: 0.6428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3fdbe1e7134bc1b640440c349ae3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>55</td></tr><tr><td>train_accuracy</td><td>0.73046</td></tr><tr><td>train_loss</td><td>0.80049</td></tr><tr><td>validation_accuracy</td><td>0.6428</td></tr><tr><td>validation_loss</td><td>1.02851</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_LR0.001_BS128</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/ou93mfod' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/ou93mfod</a><br/> View job at <a href='https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v233' target=\"_blank\">https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v233</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240415_201626-ou93mfod/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dmnkf/del/runs/ou93mfod?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f788d8b4d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='baseline', epochs=EPOCHS, batch_size=BATCH_SIZE, seed=55, track=True, run_name=f'baseline_LR{LEARNING_RATE}_BS{BATCH_SIZE}')\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SGD, without REG, without BN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialisation\n",
    "Weight initialisation refers to the process of assigning initial values to the weights of a network's neurons. This setup is crucial because it significantly influences the learning process and the model's performance. Proper initialisation can help avoid issues such as vanishing gradients, where updates to weights become insignificantly small during training, potentially halting the learning process.\n",
    "\n",
    "**Key Points:**\n",
    "- **Vanishing Gradient**: This problem can slow down the training or prevent convergence by making updates to the weights too small.\n",
    "- **Convergence Speed**: Good initialisation methods can speed up convergence by ensuring weights start at a scale that prevents early saturation of neurons.\n",
    "- **Local Minima**: Properly initialized weights can help the model avoid getting stuck in less optimal local minima during training.\n",
    "\n",
    "Standards for weight initialization have evolved based on the activation function used in the network. For networks using ReLU activations, He Initialization, which adjusts weights based on the number of incoming nodes to a neuron, is considered best practice. \n",
    "\n",
    "**PyTorch Implementation:**\n",
    "PyTorch applies He initialization for Linear and Conv layers tailored to LeakyReLU activations by default (`gain = sqrt(2 / (1 + negative_slope^2))` the negative slope being `sqrt(5)`). \n",
    "\n",
    "[Relevant PyTorch Source Code](https://arc.net/l/quote/juevbrgc)\n",
    "\n",
    "#### Experiments\n",
    "\n",
    "To better understand the impact of weight initialisation and its implication we will look at a variety of different initialisations down below and compare them. Given the initialisation done there are different expected outcomes which will then be analysed at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "class OneWeightCNN(BasicCNN):\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "\n",
    "model = OneWeightCNN()\n",
    "# check if all weights are 1\n",
    "model.initialize_weights()\n",
    "\n",
    "# Validate that all weights are set to 1\n",
    "for name, param in model.named_parameters():\n",
    "    if \"weight\" in name:  # Ensure we're only checking weights, not biases\n",
    "        assert torch.all(param == 1), f\"{name} not all ones\"\n",
    "print(\"All weights verified as 1.\")\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', seed=55, track=True, run_name='OneWeightCNN')\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "class RandomWeightCNN(BasicCNN):\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                # Initialize weights with a uniform distribution\n",
    "                nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "model = RandomWeightCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', seed=55, track=True, run_name='RandomWeightCNN')\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "class DefaultWeightCNN(BasicCNN):\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=10, layers=None):\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "model = DefaultWeightCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='weight_init', epochs=EPOCHS, batch_size=BATCH_SIZE, seed=55, track=True, run_name=\"DefaultCNN\")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot wandb weight_init report\n",
    "\n",
    "%wandb dmnkf/del/reports/weight_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results of the report we will change the default weight init of Conv2d and Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Für jedes Modell mit gegebener Anzahl Layer und Units pro Layer führe ein sorgfältiges Hyper-Parameter-Tuning durch. Untersuche, wie sich die das Training verändert bei unterschiedlicher Wahl für die Lernrate, in einer separaten Betrachtung auch für die Batch-Grösse. Achte stets darauf, dass das Training stabil läuft. Merke Dir bei jedem Training, den Loss, die Performance Metrik(en) inkl. Schätzfehler, die verwendete Anzahl Epochen, Lernrate und Batch-Grösse. Beachte: Keine Verfahren zur automatischen Hyperparameter-Suche (z.B. kein Bayesian und kein Random Parameter- Sweep Methoden) verwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/del_mc1/wandb/run-20240415_190505-aygax0d7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/aygax0d7' target=\"_blank\">LR1</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/aygax0d7' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/aygax0d7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: nan, Accuracy: 0.1000\n",
      "Validation - Epoch [1/3], Loss: nan, Accuracy: 0.1000\n",
      "Epoch [2/3], Loss: nan, Accuracy: 0.1000\n",
      "Validation - Epoch [2/3], Loss: nan, Accuracy: 0.1000\n",
      "Epoch [3/3], Loss: nan, Accuracy: 0.1000\n",
      "Validation - Epoch [3/3], Loss: nan, Accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1460c0e5577b49899c6491977f7f4e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▅▅██</td></tr><tr><td>train_accuracy</td><td>▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train_accuracy</td><td>0.1</td></tr><tr><td>train_loss</td><td>nan</td></tr><tr><td>validation_accuracy</td><td>0.1</td></tr><tr><td>validation_loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LR1</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/aygax0d7' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/aygax0d7</a><br/> View job at <a href='https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v231' target=\"_blank\">https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v231</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240415_190505-aygax0d7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dmnkf/del/runs/aygax0d7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fec01347010>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_epochs = {\n",
    "    1: 5,\n",
    "    0.1: 30,\n",
    "    0.01: 45,\n",
    "    0.00001: 100\n",
    "}\n",
    "\n",
    "for lr, epochs in lr_epochs.items():\n",
    "    model = BasicCNN()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, epochs=epochs, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='learning_rate', seed=55, track=True, run_name=f\"LR{lr}\")\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/del_mc1/wandb/run-20240415_184250-4vspr3uq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmnkf/del/runs/4vspr3uq' target=\"_blank\">BS500</a></strong> to <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmnkf/del' target=\"_blank\">https://wandb.ai/dmnkf/del</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmnkf/del/runs/4vspr3uq' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/4vspr3uq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.2946, Accuracy: 0.2038\n",
      "Validation - Epoch [1/50], Loss: 2.0561, Accuracy: 0.2649\n",
      "Epoch [2/50], Loss: 1.9627, Accuracy: 0.3010\n",
      "Validation - Epoch [2/50], Loss: 1.9033, Accuracy: 0.3233\n",
      "Epoch [3/50], Loss: 1.8513, Accuracy: 0.3410\n",
      "Validation - Epoch [3/50], Loss: 1.8213, Accuracy: 0.3542\n",
      "Epoch [4/50], Loss: 1.7826, Accuracy: 0.3677\n",
      "Validation - Epoch [4/50], Loss: 1.7656, Accuracy: 0.3795\n",
      "Epoch [5/50], Loss: 1.7330, Accuracy: 0.3860\n",
      "Validation - Epoch [5/50], Loss: 1.7232, Accuracy: 0.3902\n",
      "Epoch [6/50], Loss: 1.6932, Accuracy: 0.4012\n",
      "Validation - Epoch [6/50], Loss: 1.6898, Accuracy: 0.4075\n",
      "Epoch [7/50], Loss: 1.6597, Accuracy: 0.4132\n",
      "Validation - Epoch [7/50], Loss: 1.6622, Accuracy: 0.4174\n",
      "Epoch [8/50], Loss: 1.6309, Accuracy: 0.4234\n",
      "Validation - Epoch [8/50], Loss: 1.6358, Accuracy: 0.4264\n",
      "Epoch [9/50], Loss: 1.6061, Accuracy: 0.4334\n",
      "Validation - Epoch [9/50], Loss: 1.6126, Accuracy: 0.4303\n",
      "Epoch [10/50], Loss: 1.5839, Accuracy: 0.4423\n",
      "Validation - Epoch [10/50], Loss: 1.5946, Accuracy: 0.4367\n",
      "Epoch [11/50], Loss: 1.5632, Accuracy: 0.4490\n",
      "Validation - Epoch [11/50], Loss: 1.5766, Accuracy: 0.4418\n",
      "Epoch [12/50], Loss: 1.5449, Accuracy: 0.4554\n",
      "Validation - Epoch [12/50], Loss: 1.5571, Accuracy: 0.4492\n",
      "Epoch [13/50], Loss: 1.5275, Accuracy: 0.4622\n",
      "Validation - Epoch [13/50], Loss: 1.5438, Accuracy: 0.4563\n",
      "Epoch [14/50], Loss: 1.5114, Accuracy: 0.4668\n",
      "Validation - Epoch [14/50], Loss: 1.5304, Accuracy: 0.4608\n",
      "Epoch [15/50], Loss: 1.4959, Accuracy: 0.4733\n",
      "Validation - Epoch [15/50], Loss: 1.5168, Accuracy: 0.4648\n",
      "Epoch [16/50], Loss: 1.4812, Accuracy: 0.4790\n",
      "Validation - Epoch [16/50], Loss: 1.5087, Accuracy: 0.4690\n",
      "Epoch [17/50], Loss: 1.4677, Accuracy: 0.4830\n",
      "Validation - Epoch [17/50], Loss: 1.4938, Accuracy: 0.4759\n",
      "Epoch [18/50], Loss: 1.4549, Accuracy: 0.4886\n",
      "Validation - Epoch [18/50], Loss: 1.4845, Accuracy: 0.4772\n",
      "Epoch [19/50], Loss: 1.4426, Accuracy: 0.4936\n",
      "Validation - Epoch [19/50], Loss: 1.4700, Accuracy: 0.4828\n",
      "Epoch [20/50], Loss: 1.4305, Accuracy: 0.4976\n",
      "Validation - Epoch [20/50], Loss: 1.4607, Accuracy: 0.4857\n",
      "Epoch [21/50], Loss: 1.4193, Accuracy: 0.5029\n",
      "Validation - Epoch [21/50], Loss: 1.4521, Accuracy: 0.4896\n",
      "Epoch [22/50], Loss: 1.4089, Accuracy: 0.5061\n",
      "Validation - Epoch [22/50], Loss: 1.4419, Accuracy: 0.4936\n",
      "Epoch [23/50], Loss: 1.3986, Accuracy: 0.5093\n",
      "Validation - Epoch [23/50], Loss: 1.4328, Accuracy: 0.4971\n",
      "Epoch [24/50], Loss: 1.3881, Accuracy: 0.5131\n",
      "Validation - Epoch [24/50], Loss: 1.4262, Accuracy: 0.5014\n",
      "Epoch [25/50], Loss: 1.3789, Accuracy: 0.5165\n",
      "Validation - Epoch [25/50], Loss: 1.4157, Accuracy: 0.5013\n",
      "Epoch [26/50], Loss: 1.3698, Accuracy: 0.5193\n",
      "Validation - Epoch [26/50], Loss: 1.4069, Accuracy: 0.5059\n",
      "Epoch [27/50], Loss: 1.3604, Accuracy: 0.5230\n",
      "Validation - Epoch [27/50], Loss: 1.4015, Accuracy: 0.5094\n",
      "Epoch [28/50], Loss: 1.3512, Accuracy: 0.5256\n",
      "Validation - Epoch [28/50], Loss: 1.3957, Accuracy: 0.5084\n",
      "Epoch [29/50], Loss: 1.3430, Accuracy: 0.5295\n",
      "Validation - Epoch [29/50], Loss: 1.3877, Accuracy: 0.5099\n",
      "Epoch [30/50], Loss: 1.3345, Accuracy: 0.5329\n",
      "Validation - Epoch [30/50], Loss: 1.3796, Accuracy: 0.5130\n",
      "Epoch [31/50], Loss: 1.3258, Accuracy: 0.5357\n",
      "Validation - Epoch [31/50], Loss: 1.3719, Accuracy: 0.5161\n",
      "Epoch [32/50], Loss: 1.3186, Accuracy: 0.5394\n",
      "Validation - Epoch [32/50], Loss: 1.3654, Accuracy: 0.5207\n",
      "Epoch [33/50], Loss: 1.3103, Accuracy: 0.5413\n",
      "Validation - Epoch [33/50], Loss: 1.3611, Accuracy: 0.5228\n",
      "Epoch [34/50], Loss: 1.3032, Accuracy: 0.5454\n",
      "Validation - Epoch [34/50], Loss: 1.3565, Accuracy: 0.5223\n",
      "Epoch [35/50], Loss: 1.2964, Accuracy: 0.5465\n",
      "Validation - Epoch [35/50], Loss: 1.3495, Accuracy: 0.5263\n",
      "Epoch [36/50], Loss: 1.2887, Accuracy: 0.5491\n",
      "Validation - Epoch [36/50], Loss: 1.3472, Accuracy: 0.5225\n",
      "Epoch [37/50], Loss: 1.2816, Accuracy: 0.5533\n",
      "Validation - Epoch [37/50], Loss: 1.3392, Accuracy: 0.5295\n",
      "Epoch [38/50], Loss: 1.2755, Accuracy: 0.5552\n",
      "Validation - Epoch [38/50], Loss: 1.3311, Accuracy: 0.5307\n",
      "Epoch [39/50], Loss: 1.2679, Accuracy: 0.5572\n",
      "Validation - Epoch [39/50], Loss: 1.3299, Accuracy: 0.5337\n",
      "Epoch [40/50], Loss: 1.2617, Accuracy: 0.5595\n",
      "Validation - Epoch [40/50], Loss: 1.3260, Accuracy: 0.5328\n",
      "Epoch [41/50], Loss: 1.2552, Accuracy: 0.5612\n",
      "Validation - Epoch [41/50], Loss: 1.3167, Accuracy: 0.5385\n",
      "Epoch [42/50], Loss: 1.2486, Accuracy: 0.5647\n",
      "Validation - Epoch [42/50], Loss: 1.3142, Accuracy: 0.5362\n",
      "Epoch [43/50], Loss: 1.2426, Accuracy: 0.5654\n",
      "Validation - Epoch [43/50], Loss: 1.3100, Accuracy: 0.5383\n",
      "Epoch [44/50], Loss: 1.2369, Accuracy: 0.5684\n",
      "Validation - Epoch [44/50], Loss: 1.3013, Accuracy: 0.5436\n",
      "Epoch [45/50], Loss: 1.2310, Accuracy: 0.5698\n",
      "Validation - Epoch [45/50], Loss: 1.3001, Accuracy: 0.5425\n",
      "Epoch [46/50], Loss: 1.2247, Accuracy: 0.5748\n",
      "Validation - Epoch [46/50], Loss: 1.2939, Accuracy: 0.5458\n",
      "Epoch [47/50], Loss: 1.2191, Accuracy: 0.5745\n",
      "Validation - Epoch [47/50], Loss: 1.2920, Accuracy: 0.5432\n",
      "Epoch [48/50], Loss: 1.2136, Accuracy: 0.5780\n",
      "Validation - Epoch [48/50], Loss: 1.2874, Accuracy: 0.5455\n",
      "Epoch [49/50], Loss: 1.2081, Accuracy: 0.5789\n",
      "Validation - Epoch [49/50], Loss: 1.2831, Accuracy: 0.5461\n",
      "Epoch [50/50], Loss: 1.2024, Accuracy: 0.5814\n",
      "Validation - Epoch [50/50], Loss: 1.2840, Accuracy: 0.5457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a23ae7512a4c9681c2445c40294cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▃▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_accuracy</td><td>0.58136</td></tr><tr><td>train_loss</td><td>1.20244</td></tr><tr><td>validation_accuracy</td><td>0.5457</td></tr><tr><td>validation_loss</td><td>1.28399</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BS500</strong> at: <a href='https://wandb.ai/dmnkf/del/runs/4vspr3uq' target=\"_blank\">https://wandb.ai/dmnkf/del/runs/4vspr3uq</a><br/> View job at <a href='https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v231' target=\"_blank\">https://wandb.ai/dmnkf/del/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzY1MjYzMw==/version_details/v231</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240415_184250-4vspr3uq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dmnkf/del/runs/4vspr3uq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fec01358880>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# bs : epochs dictionary to optimize training time\n",
    "bs_epochs = {\n",
    "    1000: 100,\n",
    "    64: 30,\n",
    "    32: 25,\n",
    "    16: 15\n",
    "}\n",
    "\n",
    "\n",
    "for bs, epochs in bs_epochs.items():\n",
    "    model = BasicCNN()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='batch_size', batch_size=bs, epochs=epochs, seed=55, track=True, run_name=f\"BS{bs}\")\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "Variiere die Anzahl Layer und Anzahl Units pro Layer, um eine möglichst gute Performance zu erreichen. Falls auch CNNs (ohne Transfer-Learning) verwendet werden variiere auch Anzahl Filter, Kernel-Grösse, Stride, Padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the training of different complexities without generating too much code I wrote the create_model function which will generate the model given a configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model(num_conv_layers, conv_channels, kernel_size, stride, padding,\n",
    "                 input_channels=3, num_classes=10, input_size=32, linear_layer_sizes=[]):\n",
    "    assert len(conv_channels) == num_conv_layers, \"Length of conv_channels must match num_conv_layers\"\n",
    "    assert len(kernel_size) == num_conv_layers, \"Length of kernel_size must match num_conv_layers\"\n",
    "    assert len(stride) == num_conv_layers, \"Length of stride must match num_conv_layers\"\n",
    "    assert len(padding) == num_conv_layers, \"Length of padding must match num_conv_layers\"\n",
    "\n",
    "    layers = []\n",
    "    current_size = input_size\n",
    "    channels_in = input_channels\n",
    "\n",
    "    # Building convolutional layers\n",
    "    for i in range(num_conv_layers):\n",
    "        layers += [\n",
    "            nn.Conv2d(channels_in, conv_channels[i], kernel_size=kernel_size[i], stride=stride[i], padding=padding[i]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ]\n",
    "        # Update the current size after each conv and pooling operation\n",
    "        current_size = (current_size + 2 * padding[i] - kernel_size[i]) // stride[i] + 1\n",
    "        current_size = (current_size - 2) // 2 + 1\n",
    "        channels_in = conv_channels[i]\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "\n",
    "    # Calculating the flattened size for the first linear layer\n",
    "    current_size = current_size * current_size * channels_in\n",
    "\n",
    "    if linear_layer_sizes:\n",
    "        for i, layer_size in enumerate(linear_layer_sizes):\n",
    "            if i == 0:  # The first linear layer after flattening\n",
    "                layers.append(nn.Linear(current_size, layer_size))\n",
    "            else:\n",
    "                layers.append(nn.Linear(linear_layer_sizes[i-1], layer_size))\n",
    "            if i < len(linear_layer_sizes) - 1:  # No activation after the last specified layer before output\n",
    "                layers.append(nn.ReLU())\n",
    "        # Final layer to number of classes\n",
    "        layers.append(nn.Linear(linear_layer_sizes[-1], num_classes))\n",
    "    else:\n",
    "        layers.append(nn.Linear(current_size, num_classes))\n",
    "\n",
    "    return BasicCNN(input_channels, num_classes, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "To get started, we will explore the impact of varying the number of layers in our neural network models. We will experiment with increasing the number of convolutional and fully connected layers to observe how this affects the model's performance across different metrics.\n",
    "\n",
    "Adding more layers can potentially increase a model’s ability to learn nuanced features of the data due to a higher parameter count, which might lead to improved performance on complex tasks. However, this increase in complexity not only raises computational costs and training duration but also heightens the risk of overfitting. Overfitting can severely impair a model’s ability to generalize from the training data to unseen data, potentially resulting in poorer overall performance.\n",
    "\n",
    "To mitigate these risks, techniques such as dropout, L2 regularization, and data augmentation can be employed to enhance model generalization. (will be looked at later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter Size Experiment\n",
    "model_128_128ch = create_model(\n",
    "    num_conv_layers=2,\n",
    "    linear_layer_sizes=[128],\n",
    "    conv_channels=[128, 128],\n",
    "    kernel_size=[3, 3],\n",
    "    stride=[1, 1],\n",
    "    padding=[1, 1]\n",
    ")\n",
    "\n",
    "model_32_16ch = create_model(\n",
    "    num_conv_layers=2,\n",
    "    linear_layer_sizes=[128],\n",
    "    conv_channels=[32, 16],\n",
    "    kernel_size=[3, 3],\n",
    "    stride=[1, 1],\n",
    "    padding=[1, 1]\n",
    ")\n",
    "\n",
    "model_32_64_128ch = create_model(\n",
    "    num_conv_layers=3,\n",
    "    linear_layer_sizes=[128],\n",
    "    conv_channels=[32, 64, 128],\n",
    "    kernel_size=[3, 3, 3],\n",
    "    stride=[1, 1, 1],\n",
    "    padding=[1, 1, 1]\n",
    ")\n",
    "\n",
    "model_128_256_512ch = create_model(\n",
    "    num_conv_layers=3,\n",
    "    linear_layer_sizes=[128],\n",
    "    conv_channels=[128, 256, 512],\n",
    "    kernel_size=[3, 3, 3],\n",
    "    stride=[1, 1, 1],\n",
    "    padding=[1, 1, 1]\n",
    ")\n",
    "\n",
    "model_96_256_512_1024ch = create_model(\n",
    "    num_conv_layers=4,\n",
    "    linear_layer_sizes=[128],\n",
    "    conv_channels=[96, 256, 512, 1024],\n",
    "    kernel_size=[3, 3, 3, 3],\n",
    "    stride=[1, 1, 1, 1],\n",
    "    padding=[1, 1, 1, 1]\n",
    ")\n",
    "\n",
    "\n",
    "layer_dict = {\n",
    "    \"128_128ch_128L\": model_128_128ch,\n",
    "    \"32_16ch_128L\": model_32_16ch,\n",
    "    \"32_64_128ch_128L\": model_32_64_128ch,\n",
    "    \"128_256_512ch_128L\": model_128_256_512ch,\n",
    "    \"96_256_512_1024ch_128L\": model_96_256_512_1024ch,\n",
    "}\n",
    "\n",
    "for name, model in layer_dict.items():\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='filter_size', seed=55, track=True, run_name=name)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Size\n",
    "\n",
    "The kernel size in convolutional layers plays a pivotal role in determining how effectively a CNN captures spatial information and feature granularity from input images. Larger kernels encompass a broader view of the input at each convolution step, enabling the model to capture more global features within fewer layers. This can be advantageous for identifying larger patterns but may increase computational cost and risk overfitting due to the greater number of parameters. Conversely, smaller kernels focus on finer details and require more layers to achieve a comparable field of view, promoting sensitivity to local features without drastically increasing the parameter count.\n",
    "\n",
    "#### Expectation for CIFAR-10\n",
    "\n",
    "Given that CIFAR-10 consists of relatively small images (32x32 pixels) with objects like animals and vehicles that often occupy much of the frame, smaller to medium kernel sizes (e.g., 3x3 or 5x5) are typically preferred. These sizes are likely to be optimal for balancing detailed feature extraction with computational efficiency while avoiding the unnecessary complexity that larger kernels might introduce. For CIFAR-10, we would expect smaller kernels to perform better, allowing the network to learn detailed textures and shapes effectively, leading to superior generalization on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5x5kern = create_model(num_conv_layers=2, \n",
    "                             linear_layer_sizes=[128],\n",
    "                             conv_channels=[32, 96],\n",
    "                             kernel_size=[5, 5], \n",
    "                             stride=[1, 1],\n",
    "                             padding=[2, 2])\n",
    "\n",
    "model_7x7kern = create_model(num_conv_layers=2, \n",
    "                             linear_layer_sizes=[128],\n",
    "                             conv_channels=[32, 96],\n",
    "                             kernel_size=[7, 7],\n",
    "                             stride=[1, 1],\n",
    "                             padding=[3, 3])\n",
    "\n",
    "model_15x15kern = create_model(num_conv_layers=2, \n",
    "                             linear_layer_sizes=[128],\n",
    "                             conv_channels=[32, 96],\n",
    "                             kernel_size=[15, 15],\n",
    "                             stride=[1, 1],\n",
    "                             padding=[7, 7])\n",
    "\n",
    "kernel_dict = {\n",
    "    \"5x5K_1S_2P\": model_5x5kern,\n",
    "    \"7x7K_1S_3P\": model_7x7kern,\n",
    "    \"15x15K_1S_7P\": model_15x15kern,\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in kernel_dict.items():\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='kernel_size', seed=55, track=True, run_name=name)\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "Stride defines the step size at which the kernel moves across the input image during a convolution operation in a convolutional neural network (CNN). Adjusting the stride affects how quickly the spatial dimensions of the output feature maps are reduced. A larger stride results in more aggressive down-sampling, reducing the output size quickly, which can decrease the computational load and speed up the processing time. However, a larger stride may also cause the network to miss finer details in the input, potentially leading to a loss in accuracy if important features are skipped. Conversely, a smaller stride tends to preserve spatial resolution better, capturing more detailed information but at the cost of increased computational complexity.\n",
    "\n",
    "#### Expectation for CIFAR-10\n",
    "\n",
    "For CIFAR-10, where the images are relatively small (32x32 pixels) and the important features such as edges and color blocks are closely packed, a smaller stride (e.g., stride of 1) is generally more appropriate. This setting helps in capturing fine details without losing important spatial information, essential for accurately classifying objects in such small images. Using a stride of 1 maximizes the amount of learned detail, enhancing the model's ability to distinguish between similar categories by focusing on subtle differences in features. This approach is likely to yield better performance on the CIFAR-10 dataset by preserving critical information during the convolution phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Stride2 = create_model(num_conv_layers=2, \n",
    "                             conv_channels=[32, 96],\n",
    "                             linear_layer_sizes=[128],\n",
    "                             kernel_size=[3, 3], \n",
    "                             stride=[2, 2], \n",
    "                             padding=[1, 1])\n",
    "\n",
    "model_Stride3 = create_model(num_conv_layers=2, \n",
    "                             conv_channels=[32, 96],\n",
    "                             linear_layer_sizes=[128],\n",
    "                             kernel_size=[3, 3], \n",
    "                             stride=[3, 3], \n",
    "                             padding=[1, 1])\n",
    "\n",
    "# todo add best kernel size from last check\n",
    "\n",
    "stride_dict = {\n",
    "    \"3x3K_2S_1P\": model_Stride2,\n",
    "    \"3x3K_3S_1P\": model_Stride3,\n",
    "}\n",
    "\n",
    "for name, model in stride_dict.items():\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='stride_size', seed=55, track=True, run_name=name)\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0pad = create_model(num_conv_layers=2, \n",
    "                          conv_channels=[32, 96],\n",
    "                          linear_layer_sizes=[128],\n",
    "                          kernel_size=[3, 3], \n",
    "                          stride=[1, 1], \n",
    "                          padding=[0, 0])\n",
    "\n",
    "# todo add best kernel\n",
    "\n",
    "padding_dict = {\n",
    "    \"3x3K_1S_0P\": model_0pad,\n",
    "}\n",
    "\n",
    "for name, model in padding_dict.items():\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='padding', seed=55, track=True, run_name=name)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Layers Experiment\n",
    "# no addtional linear layer\n",
    "model_1Lin = create_model(num_conv_layers=2, linear_layer_sizes=[], conv_channels=[32, 96],\n",
    "                          kernel_size=[3, 3], stride=[1, 1], padding=[1, 1])\n",
    "model_3Lin = create_model(num_conv_layers=2, linear_layer_sizes=[256, 128], conv_channels=[32, 96],\n",
    "                          kernel_size=[3, 3], stride=[1, 1], padding=[1, 1])\n",
    "\n",
    "linear_dict = {\n",
    "    \"32_96CH\": model_1Lin,\n",
    "    \"32_96CH_256_128L\": model_3Lin,\n",
    "}\n",
    "\n",
    "for name, model in linear_dict.items():\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='linear_layers', seed=55, track=True, run_name=name)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Ziehe nun verschiedene Regularisierungsmethoden bei den MLP Layern in Betracht:\n",
    "a. L1/L2 Weight Penalty\n",
    "b. Dropout\n",
    "Evaluiere den Nutzen der Regularisierung, auch unter Berücksichtigung verschiedener Regularisierungsstärken.\n",
    "Beschreibe auch kurz, was allgemein das Ziel von Regularisierungsmethoden ist (Regularisierung im Allgemeinen, sowie auch Idee der einzelnen Methoden). Inwiefern wird dieses Ziel im gegebenen Fall erreicht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "weight_decays = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for weight_decay in weight_decays:\n",
    "    # BasicCNN with LR 0.1 did overfit\n",
    "    model = BasicCNN()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=weight_decay) \n",
    "\n",
    "    trainer = ModelTrainer(model=model, epochs=50, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='regularization', seed=55, track=True, run_name=f'REGL2_{weight_decay}')\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CNNRegL1(BasicCNN):\n",
    "    def __init__(self, l1_strength=0.0005, **kwargs):\n",
    "        super(CNNRegL1, self).__init__(**kwargs)\n",
    "        self.l1_strength = l1_strength\n",
    "\n",
    "    def l1_penalty(self):\n",
    "        \"\"\"\n",
    "        Calculate the L1 penalty for the model's weights only, excluding biases.\n",
    "        This method iterates over all parameters that require gradients and have more than one dimension,\n",
    "        which generally corresponds to the weights of the model.\n",
    "        \"\"\"\n",
    "        l1_norm = sum(p.abs().sum() for p in self.parameters() if p.requires_grad and len(p.shape) > 1)\n",
    "        return self.l1_strength * l1_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "l1_strengths = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for l1_strength in l1_strengths:\n",
    "    model = CNNRegL1(input_channels=3, num_classes=10, l1_strength=l1_strength)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "\n",
    "    trainer = ModelTrainer(model=model, epochs=50, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='regularization', seed=55, track=True, run_name=f'REGL1_{l1_strength}')\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rethink this\n",
    "class DropoutCNN(BasicCNN):\n",
    "    def __init__(self, input_channels=3, num_classes=10, dropout_rate=0.5):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            # Flattening the output for the fully connected layer\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Fully connected layer\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "        \n",
    "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model = DropoutCNN(input_channels=3, num_classes=10, dropout_rate=dropout_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='regularization', epochs=EPOCHS, batch_size=BATCH_SIZE, seed=55, track=True, run_name=f'DropoutCNN_{dropout_rate}')\n",
    "    trainer.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Batchnorm  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormCNN(BasicCNN):\n",
    "    def __init__(self, input_channels=3, num_classes=10):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(512 * 4 * 4, num_classes)\n",
    "        )\n",
    "        super().__init__(input_channels, num_classes, layers)\n",
    "            \n",
    "\n",
    "model = BatchNormCNN(input_channels=3, num_classes=10)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='batchnorm', seed=55, track=True, run_name='BatchNormCNN')\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adam_learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "adam_weight_decays = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for lr in adam_learning_rates:\n",
    "    model = BasicCNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='adam', seed=55, track=True, run_name=f'Adam_LR{lr}')\n",
    "    trainer.run()\n",
    "\n",
    "for weight_decay in adam_weight_decays:\n",
    "    model = BasicCNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n",
    "\n",
    "    trainer = ModelTrainer(model=model, train_dataset=train_dataset, test_dataset=test_dataset, optimizer=optimizer, experiment='adam', seed=55, track=True, run_name=f'Adam_WD{weight_decay}')\n",
    "    trainer.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
